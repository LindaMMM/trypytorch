{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caac60f7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42afef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "import pathlib as pl\n",
    "from typing import Tuple\n",
    "from typing import List\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"LSTM model for time series prediction\"\"\"\n",
    "    def __init__(self,input_size,hidden_size=128,num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        # store parameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # RNN Layer (notation: LSTM \\in RNN)\n",
    "        self.lstm = nn.LSTM(\n",
    "        input_size=input_size,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                dropout=dropout if num_layers > 1 else 0,\n",
    "                batch_first=True\n",
    "                )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # linear layer for output\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        out = self.dropout(last_output)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    \"\"\"GRU model for time series prediction\"\"\"\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.2):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.gru(x)\n",
    "        last_output = gru_out[:, -1, :]\n",
    "        out = self.dropout(last_output)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "    \n",
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for time series data\"\"\"\n",
    "    def __init__(self, X, y, sequence_length=10):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X) - self.sequence_length\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.X[idx:idx + self.sequence_length],\n",
    "            self.y[idx + self.sequence_length]\n",
    "        )\n",
    "    \n",
    "\n",
    "\n",
    "class PyTrainer:\n",
    "    def __init__(self, model, device=device):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def prepare_data(self, X, y, sequence_length=10, batch_size=32):\n",
    "        \"\"\"Prepare data for training\"\"\"\n",
    "        # Scale features\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Create dataset and dataloader\n",
    "        dataset = TimeSeriesDataset(X_scaled, y, sequence_length)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        return dataloader, self.scaler\n",
    "    \n",
    "\n",
    "    def __mae(self, y_pred, y_true):\n",
    "        return torch.mean(torch.abs(y_pred - y_true))\n",
    "\n",
    "    def __rmse(self, y_pred, y_true):\n",
    "        return torch.sqrt(torch.mean((y_pred - y_true) ** 2))\n",
    "\n",
    "    def __train_epoch(self, model, dataloader, optimizer, criterion):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for X, y in dataloader:\n",
    "            batch_X, batch_y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(batch_X)\n",
    "            loss = criterion(y_pred.squeeze(), batch_y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        return total_loss / len(dataloader)\n",
    "\n",
    "    def __evaluate(self, model, dataloader, optimizer, criterion):\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        y_preds, y_trues = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X, y in dataloader:\n",
    "                batch_X, batch_y = X.to(device), y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                y_pred = model(batch_X)\n",
    "                total_loss += criterion(y_pred.squeeze(), batch_y).item()\n",
    "                y_preds.append(y_pred)\n",
    "                y_trues.append(y)\n",
    "\n",
    "                \n",
    "        y_preds = torch.cat(y_preds)\n",
    "        y_trues = torch.cat(y_trues)\n",
    "\n",
    "        return (\n",
    "            total_loss / len(dataloader),\n",
    "            self.__mae(y_preds, y_trues).item(),\n",
    "            self.__rmse(y_preds, y_trues).item()\n",
    "        )\n",
    "    def train(self, train_loader,\n",
    "                val_loader,\n",
    "                epochs,\n",
    "                lr,\n",
    "                patience):\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=patience/2, factor=0.5)\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        val_maes = []\n",
    "        val_rmses = []\n",
    "        val_lrs = []\n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "        \n",
    "            train_loss = self.__train_epoch(self.model, train_loader, optimizer, criterion)\n",
    "            train_losses.append(train_loss)\n",
    "            # Validation\n",
    "            if val_loader is not None:\n",
    "\n",
    "                val_loss, val_mae, val_rmse = self.__evaluate(self.model, val_loader,optimizer,criterion )\n",
    "                \n",
    "                val_losses.append(val_loss)\n",
    "                val_maes.append(val_mae)\n",
    "                val_rmses.append(val_rmse)\n",
    "                current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "                val_lrs.append(current_lr)\n",
    "                # Learning rate scheduling\n",
    "                scheduler.step(val_loss)\n",
    "                \n",
    "                # Early stopping\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                \n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "                \n",
    "                \n",
    "                if epoch % 2 == 0:\n",
    "                    print(\n",
    "                        f\"Epoch {epoch+1:02d} | \"\n",
    "                        f\"LR: {current_lr:.1e} | \"\n",
    "                        f\"Train Loss: {train_loss:.4f} | \"\n",
    "                        f\"Val Loss: {val_loss:.4f} | \"\n",
    "                        f\"MAE: {val_mae:.4f} | \"\n",
    "                        f\"RMSE: {val_rmse:.4f}\"\n",
    "                    )\n",
    "                    # print(f\"Epoch {epoch}: Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "            else:\n",
    "                if epoch % 2 == 0:\n",
    "                    print(f\"Epoch {epoch}: Train Loss: {train_loss:.6f}\")    \n",
    "        \n",
    "        return train_losses, val_losses , val_lrs, val_maes, val_rmses\n",
    "        \n",
    "    \n",
    "\n",
    "class DeepLearningStudy:\n",
    "    def __init__(self, n_trials=100, timeout=3600):\n",
    "        self.n_trials = n_trials\n",
    "        self.timeout = timeout\n",
    "        self.best_params = {}\n",
    "        self.best_scores = {}\n",
    "        self.study = None\n",
    "\n",
    "    def create_study(self, study_name, direction='minimize'):\n",
    "        \"\"\"Create Optuna study for optimization\"\"\"\n",
    "        sampler = TPESampler(seed=42)\n",
    "        pruner = MedianPruner(n_startup_trials=5, n_warmup_steps=10)\n",
    "        \n",
    "        self.study = optuna.create_study(\n",
    "            study_name=study_name,\n",
    "            direction=direction,\n",
    "            sampler=sampler,\n",
    "            pruner=pruner\n",
    "        )\n",
    "        \n",
    "        return self.study\n",
    "    \n",
    "    def get_model_params(self, model_type, trial):\n",
    "        \"\"\"Get hyperparameters for specific model type\"\"\"\n",
    "        if model_type == 'lstm':\n",
    "            return self.suggest_lstm_params(trial)\n",
    "        elif model_type == 'gru':\n",
    "            return self.suggest_gru_params(trial)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "        \n",
    "    def suggest_lstm_params(self, trial):\n",
    "        \"\"\"Suggest hyperparameters for LSTM model\"\"\"\n",
    "        params = {\n",
    "            'hidden_size': trial.suggest_categorical('hidden_size', [64, 128, 256, 512]),\n",
    "            'num_layers': trial.suggest_int('num_layers', 1, 4),\n",
    "            'dropout': trial.suggest_float('dropout', 0.1, 0.5),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),\n",
    "            'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64, 128]),\n",
    "            'sequence_length': trial.suggest_int('sequence_length', 10, 30),\n",
    "            'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n",
    "        }\n",
    "        return params\n",
    "    \n",
    "    def suggest_gru_params(self, trial):\n",
    "        \"\"\"Suggest hyperparameters for GRU model\"\"\"\n",
    "        params = {\n",
    "            'hidden_size': trial.suggest_categorical('hidden_size', [64, 128, 256, 512]),\n",
    "            'num_layers': trial.suggest_int('num_layers', 1, 4),\n",
    "            'dropout': trial.suggest_float('dropout', 0.1, 0.5),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),\n",
    "            'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64, 128]),\n",
    "            'sequence_length': trial.suggest_int('sequence_length', 10, 30),\n",
    "            'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n",
    "        }\n",
    "        return params\n",
    "    \n",
    "    \n",
    "    def optimize_model(self, model_type, X_train, y_train, X_val, y_val, \n",
    "                      input_size, study_name=None, max_epochs=50, patience=10):\n",
    "        \"\"\"Optimize hyperparameters for a specific model type\"\"\"\n",
    "        if study_name is None:\n",
    "            study_name = f\"{model_type}_optimization\"\n",
    "        \n",
    "        # Create study\n",
    "        study = self.create_study(study_name, direction='minimize')\n",
    "        \n",
    "        # Run optimization\n",
    "        study.optimize(\n",
    "            lambda trial: self.objective_function(\n",
    "                trial, model_type, X_train, y_train, X_val, y_val,\n",
    "                input_size, max_epochs, patience\n",
    "            ),\n",
    "            n_trials=self.n_trials,\n",
    "            timeout=self.timeout\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        self.best_params[model_type] = study.best_params\n",
    "        self.best_scores[model_type] = study.best_value\n",
    "        \n",
    "        print(f\"\\n{model_type.upper()} Optimization Results:\")\n",
    "        print(f\"Best Score: {study.best_value:.6f}\")\n",
    "        print(f\"Best Parameters: {study.best_params}\")\n",
    "        \n",
    "        return study\n",
    "    \n",
    "    def create_model(self, model_type, input_size, params):\n",
    "        \"\"\"Create model with given parameters\"\"\"\n",
    "        if model_type == 'lstm':\n",
    "            return LSTMModel(\n",
    "                input_size=input_size,\n",
    "                hidden_size=params['hidden_size'],\n",
    "                num_layers=params['num_layers'],\n",
    "                dropout=params['dropout']\n",
    "            )\n",
    "        elif model_type == 'gru':\n",
    "            return GRUModel(\n",
    "                input_size=input_size,\n",
    "                hidden_size=params['hidden_size'],\n",
    "                num_layers=params['num_layers'],\n",
    "                dropout=params['dropout']\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "        \n",
    "\n",
    "    def objective_function(self, trial, model_type, X_train, y_train, X_val, y_val, \n",
    "                          input_size, max_epochs=50, patience=10):\n",
    "        \"\"\"Objective function for Optuna optimization\"\"\"\n",
    "        try:\n",
    "            # Get hyperparameters\n",
    "            params = self.get_model_params(model_type, trial)\n",
    "            \n",
    "            # Create model\n",
    "            model = self.create_model(model_type, input_size, params)\n",
    "            \n",
    "            # Create trainer\n",
    "        \n",
    "            trainer = PyTrainer(model)\n",
    "            \n",
    "            # Prepare data\n",
    "            train_loader, scaler = trainer.prepare_data(\n",
    "                X_train, y_train,\n",
    "                params['sequence_length'],\n",
    "                params['batch_size']\n",
    "            )\n",
    "            \n",
    "            # Create validation data\n",
    "            val_loader, _ = trainer.prepare_data(\n",
    "                X_val, y_val,\n",
    "                params['sequence_length'],\n",
    "                params['batch_size']\n",
    "            )\n",
    "            \n",
    "            # Train model\n",
    "            train_losses, val_losses = trainer.train(\n",
    "                train_loader,\n",
    "                val_loader=val_loader,\n",
    "                epochs=max_epochs,\n",
    "                lr=params['learning_rate'],\n",
    "                patience=patience\n",
    "            )\n",
    "            \n",
    "            # Get best validation loss\n",
    "            best_val_loss = min(val_losses) if val_losses else float('inf')\n",
    "            \n",
    "            # Report intermediate value for pruning\n",
    "            trial.report(best_val_loss, step=len(val_losses))\n",
    "            \n",
    "            return best_val_loss\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Trial failed: {str(e)}\")\n",
    "            return float('inf')\n",
    "\n",
    "    def optimize_all_models(self, X_train, y_train, X_val, y_val, input_size,\n",
    "            model_types=['lstm', 'gru'], max_epochs=50, patience=10):\n",
    "        \"\"\"Optimize hyperparameters for all model types\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for model_type in model_types:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Optimizing {model_type.upper()} model...\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            try:\n",
    "                study = self.optimize_model(\n",
    "                    model_type, X_train, y_train, X_val, y_val,\n",
    "                    input_size, max_epochs, max_epochs, patience\n",
    "                )\n",
    "                results[model_type] = {\n",
    "                    'study': study,\n",
    "                    'best_params': study.best_params,\n",
    "                    'best_score': study.best_value\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"Error optimizing {model_type}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a80f040e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"Charge des fichiers de données\"\"\"\n",
    "    def __init__(self, data_path: str=\"../data/\"):\n",
    "        self.datapath = data_path\n",
    "        self.train_path = pl.Path(data_path) / \"train.csv\"\n",
    "        self.test_path = pl.Path(data_path) / \"test.csv\"\n",
    "        self.train_labels_path = pl.Path(data_path) / \"train_labels.csv\"\n",
    "        self.target_pairs_path = pl.Path(data_path) / \"target_pairs.csv\"\n",
    "        \n",
    "    def load_data(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        train_df = pd.read_csv(self.train_path, index_col='date_id')\n",
    "        test_df = pd.read_csv(self.test_path, index_col='date_id')\n",
    "        train_labels_df = pd.read_csv(self.train_labels_path, index_col='date_id')\n",
    "        target_pairs_df = pd.read_csv(self.target_pairs_path)\n",
    "        return train_df, test_df, train_labels_df, target_pairs_df\n",
    "\n",
    "class PreProcess:\n",
    "    \"\"\"PreProcess - regroupement des données par catégory\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_train_info(self, df):\n",
    "        \"\"\" Construction d'une data des entêtes de colonnes.\n",
    "        Args:\n",
    "            df (pd.DataFrame): Input dataframe d'entrainement.\n",
    "        Returns:\n",
    "            pd.DataFrame: Détail des informations de chaque colonne.\n",
    "        \"\"\"\n",
    "        df_names = df.columns\n",
    "        # Fonction pour nettoyer et split les noms\n",
    "        def clean_and_split(name):\n",
    "            name = name.replace(\"open_interest\", \"open interest\")\n",
    "            name = name.replace(\"settlement_price\", \"settlement price\")\n",
    "            name = name.replace(\"US_Stock\", \"US Stock\")\n",
    "            name = name.replace(\"adj_close\", \"Close\")\n",
    "            name = name.replace(\"adj_\", \"adjusted \")\n",
    "            name = name.replace(\"-\", \"_\")\n",
    "            return name.split(\"_\")\n",
    "\n",
    "        # Création du DataFrame d'infos\n",
    "        df_info = pd.DataFrame(\n",
    "            {\n",
    "            'Column': df_names,\n",
    "            'Split': [clean_and_split(name) for name in df_names]\n",
    "        })\n",
    "\n",
    "        df_info['Category'] = df_info['Split'].apply(lambda x: x[0])    \n",
    "        df_info['Ticker'] = df_info['Split'].apply(\n",
    "        lambda x: \"_\".join(x[1:-1]) if len(x) > 2 else x[-1] if len(x) == 2 else \"\"\n",
    "        )\n",
    "        df_info['Type'] = df_info['Split'].apply(lambda x: x[-1])\n",
    "\n",
    "        # Nettoyage final\n",
    "        df_info['Ticker'] = df_info.apply(\n",
    "            lambda row: row['Type'] if row['Ticker'] == \"\" else row['Ticker'], axis=1\n",
    "        )\n",
    "        df_info['Column_Id'] = df_info.index + 1\n",
    "\n",
    "        # Sélection des colonnes finales\n",
    "        df_info = df_info[['Column_Id', 'Column', 'Category', 'Ticker', 'Type']]    \n",
    "        return df_info\n",
    "    \n",
    "    def get_preprocess_data(self, df, cond):\n",
    "        # Fonction pour obtenir les données prétraitées en fonction de la condition\n",
    "        if cond.Column.size > 0:\n",
    "            return df[cond.Column.values[0]]\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def preprocess(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df_processed = df.copy()\n",
    "        df_processed = df_processed.drop(columns=['is_score'], errors='ignore')\n",
    "        df_info = self.get_train_info(df_processed)\n",
    "\n",
    "\n",
    "        df_processed.reset_index(inplace=True)\n",
    "        # On renomme la date_id en date\n",
    "        df_processed = df_processed.rename({'date_id': 'date'}, axis='columns')\n",
    "        # Initialisation du DataFrame résultat\n",
    "        result = pd.DataFrame(columns=['date', 'id', 'close', 'open', 'high', 'low', 'volume', 'sprice', 'interest'])\n",
    "        \n",
    "        for  Category  in df_info.groupby('Category').groups.keys():\n",
    "            txtCategory=Category.replace(' ','_')\n",
    "            for label in df_info[(df_info.Category==Category)].groupby('Ticker').groups.keys():\n",
    "                temp_df = pd.DataFrame()\n",
    "                temp_df['date'] = df_processed['date']\n",
    "                temp_df['id'] = f'{txtCategory}_{label}'\n",
    "\n",
    "                if Category in ['FX','LME']:\n",
    "                    temp_df['close'] = df_processed[df_info[(df_info.Category==Category) & (df_info.Ticker==label)].Column.values[0]]\n",
    "                    temp_df['open'] = None\n",
    "                    temp_df['high'] = None\n",
    "                    temp_df['low'] = None\n",
    "                    temp_df['volume'] = None\n",
    "                    temp_df['sprice'] = None\n",
    "                    temp_df['interest'] = None\n",
    "                else:\n",
    "                    temp_df['close'] = self.get_preprocess_data(df_processed,df_info[(df_info.Category==Category) & (df_info.Ticker==label) & (df_info.Type.isin(['Close', 'adjusted close']))])\n",
    "                    temp_df['open'] = self.get_preprocess_data(df_processed,df_info[(df_info.Category==Category) & (df_info.Ticker==label) & (df_info.Type.isin(['Open','adjusted open']))])\n",
    "                    temp_df['high'] = self.get_preprocess_data(df_processed,df_info[(df_info.Category==Category) & (df_info.Ticker==label) & (df_info.Type.isin(['High','adjusted high']))])\n",
    "                    temp_df['low'] = self.get_preprocess_data(df_processed,df_info[(df_info.Category==Category) & (df_info.Ticker==label) & (df_info.Type.isin(['Low','adjusted low']))])\n",
    "                    temp_df['volume'] = self.get_preprocess_data(df_processed,df_info[(df_info.Category==Category) & (df_info.Ticker==label) & (df_info.Type.isin(['Volume', 'adjusted volume']))])\n",
    "                    temp_df['sprice'] = self.get_preprocess_data(df_processed,df_info[(df_info.Category==Category) & (df_info.Ticker==label) & (df_info.Type.isin(['settlement price','adjusted settlement price']))])\n",
    "                    temp_df['interest'] = self.get_preprocess_data(df_processed,df_info[(df_info.Category==Category) & (df_info.Ticker==label) & (df_info.Type.isin(['open interest','adjusted open interest']))])\n",
    "                result = pd.concat([result, temp_df], ignore_index=True)\n",
    "        \n",
    "        # Réinitialiser l'index\n",
    "        result = result.reset_index(drop=True)\n",
    "        \n",
    "        # Trier par date et id \n",
    "        result = result.sort_values(['date', 'id']).reset_index(drop=True)\n",
    "        \n",
    "        return result  \n",
    "    \n",
    "class FeatureEngineer:\n",
    "    \"\"\"Creation des fonctions \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def add_lag_features(self,\n",
    "        df: pd.DataFrame, \n",
    "        lags: List[int], \n",
    "        date_col: str = 'date'\n",
    "        ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Add lag features for specified columns and lags.\n",
    "        \"\"\"\n",
    "        df = df.sort_values(date_col)\n",
    "        cols = set(df.columns)\n",
    "        cols.remove('id')\n",
    "        cols.remove('date')\n",
    "        for col in cols:\n",
    "            for lag in lags:\n",
    "                df[f'{col}_lag{lag}'] = df.groupby('id')[col].shift(lag)\n",
    "        return df\n",
    "    \n",
    "    def add_rolling_features(self,\n",
    "        df: pd.DataFrame,\n",
    "        windows: List[int],\n",
    "        date_col: str = 'date') -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "        Add rolling mean and std features for specified columns and windows.\n",
    "        \"\"\"\n",
    "        df = df.sort_values(date_col)\n",
    "        cols = set(df.columns)\n",
    "        cols.remove('id')\n",
    "        cols.remove('date')\n",
    "        for col in cols:\n",
    "            for window in windows:\n",
    "                df[f'{col}_rollmean{window}'] = df.groupby('id')[col].transform(lambda x: x.rolling(window, min_periods=1).mean())\n",
    "                df[f'{col}_rollstd{window}'] = df.groupby('id')[col].transform(lambda x: x.rolling(window, min_periods=1).std())\n",
    "                df[f'{col}_rollmin{window}'] = df.groupby('id')[col].transform(lambda x: x.rolling(window, min_periods=1).min())\n",
    "                df[f'{col}_rollmax{window}'] = df.groupby('id')[col].transform(lambda x: x.rolling(window, min_periods=1).max())\n",
    "        return df\n",
    "    \n",
    "    def prepare_features(self, df: pd.DataFrame) ->pd.DataFrame:\n",
    "        \"\"\"Engineer features for training and testing data\"\"\"\n",
    "        try:\n",
    "            # add lag\n",
    "            df_result = df.copy()\n",
    "            df_result = self.add_lag_features(df_result, lags=[1, 2, 3, 5, 7])  \n",
    "            # Add rolling features\n",
    "            df_result = self.add_rolling_features(df_result, windows=[5, 10, 15])\n",
    "            # Handle missing values\n",
    "            df_result = df_result.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "\n",
    "            return df_result\n",
    "        except Exception as e:\n",
    "            print(f\"Feature preparation failed: {e}\")\n",
    "            raise\n",
    "        \n",
    "class FeatureTarget:\n",
    "    \"\"\"Class to handle target feature engineering\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def prepare_targets(self, train_labels_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Prepare target information from pairs DataFrame.\n",
    "        Args:\n",
    "            pairs (pd.DataFrame): DataFrame containing 'pair' column.   \n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with target information including price_1, price_2, and is_pair.\n",
    "        \"\"\"\n",
    "        target_cols = [col for col in train_labels_df.columns if col not in ['timestamp', 'id']]\n",
    "        display(target_cols)\n",
    "        target_values = train_labels_df[target_cols]\n",
    "        return target_values, target_cols\n",
    "    \n",
    "    def prepare_targets_info(self, pairs: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Prepare target information from pairs DataFrame.\n",
    "        Args:\n",
    "            pairs (pd.DataFrame): DataFrame containing 'pair' column.\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with target information including price_1, price_2, and is_pair.\n",
    "        \"\"\"\n",
    "\n",
    "        target_definitions = pairs[\"pair\"].str.split(\" - \", expand=True)\n",
    "        target_info = pairs.copy()\n",
    "\n",
    "        # Colonnes price_1 et price_2 (équivalent aux colonnes [,1] et [,2])\n",
    "        target_info[\"price_1\"] = target_definitions[0]\n",
    "        target_info[\"price_2\"] = target_definitions[1]\n",
    "\n",
    "        # is.pair = second élément non vide\n",
    "        target_info['is_pair'] = target_info['price_2'].apply(lambda x:x is not None)\n",
    "\n",
    "        # Retirer la colonne \"pair\"\n",
    "        target_info = target_info.drop(columns=[\"pair\"])\n",
    "        return target_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3f5bf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataLoader = DataLoader()\n",
    "featureTarget = FeatureTarget()\n",
    "featureEngineer = FeatureEngineer()\n",
    "preProcess = PreProcess()\n",
    "train_df, test_df, train_labels_df, target_pairs_df = dataLoader.load_data()\n",
    "train_df_process = preProcess.preprocess(train_df)\n",
    "test_df_process = preProcess.preprocess(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0bc878",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_feature = featureEngineer.prepare_features(train_df_process)\n",
    "test_df_feature = featureEngineer.prepare_features(test_df_process)\n",
    "display(train_df_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a90f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_values,target_cols = featureTarget.prepare_targets(train_labels_df)\n",
    "display(target_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94c0b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_curves(train_losses, val_losses, target_name, model_name, save_path=None):\n",
    "    \"\"\"Plot training and validation loss curves\"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    epochs = range(len(train_losses))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, 'b-', label='Training Loss', alpha=0.8)\n",
    "    if val_losses:\n",
    "        plt.plot(epochs, val_losses, 'r-', label='Validation Loss', alpha=0.8)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'{target_name} - {model_name}\\nTraining Curves')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    if val_losses:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, np.array(train_losses) - np.array(val_losses), 'g-', alpha=0.8)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Train Loss - Val Loss')\n",
    "        plt.title(f'{target_name} - {model_name}\\nLoss Difference')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "def plot_evaling_curves(val_lrs, val_mae, val_rmse, target_name, model_name, save_path=None):\n",
    "    \"\"\"Plot training and validation loss curves\"\"\"\n",
    "    \n",
    "    epochs = range(len(val_lrs))\n",
    "\n",
    "    fig, ax = plt.subplots(1,3,figsize=(16, 6))\n",
    "    ax[0].plot(epochs, val_lrs, 'b-', label='Rate learnning', alpha=0.8)\n",
    "    ax[1].plot(epochs, val_mae, 'r-', label='MAE', alpha=0.8)\n",
    "    ax[2].plot(epochs, val_rmse, 'g-', label='RMSE', alpha=0.8)\n",
    "\n",
    "    ax[0].grid(True, alpha=0.3)\n",
    "    ax[1].grid(True, alpha=0.3)\n",
    "    ax[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    ax[0].set_facecolor('#f0f0ff')\n",
    "    ax[1].set_facecolor('#f0f0ff')\n",
    "    ax[2].set_facecolor('#f0f0ff')\n",
    "\n",
    "    ax[0].set_title(\"Rate learnning\")\n",
    "    ax[1].set_title(\"MAE\")\n",
    "    ax[2].set_title(\"RMSE\")\n",
    "\n",
    "    plt.suptitle(f'{target_name} - {model_name}\\nEval')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return plt.gcf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

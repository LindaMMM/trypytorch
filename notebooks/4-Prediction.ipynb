{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13f15e06",
   "metadata": {},
   "source": [
    "# Prédiction\n",
    "## Auteur Linda MARTIN\n",
    "On a les parametres pour sauvegarder les models. on peut tester les prédictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ed41a185",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### import libraries\n",
    "import torch\n",
    "import joblib\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "import polars as pl\n",
    "from typing import Tuple\n",
    "from typing import List\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import torch.optim as optim\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "eca3baa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoaderData:\n",
    "    \"\"\"Charge des fichiers de données\"\"\"\n",
    "    def __init__(self, data_path: str=\"../data/\"):\n",
    "        self.datapath = data_path\n",
    "        self.train_path = Path(data_path) / \"train.csv\"\n",
    "        self.test_path = Path(data_path) / \"test.csv\"\n",
    "        self.train_labels_path = Path(data_path) / \"train_labels.csv\"\n",
    "        self.target_pairs_path = Path(data_path) / \"target_pairs.csv\"\n",
    "        \n",
    "    def load_data(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        train_df = pd.read_csv(self.train_path, index_col='date_id')\n",
    "        test_df = pd.read_csv(self.test_path, index_col='date_id')\n",
    "        train_labels_df = pd.read_csv(self.train_labels_path, index_col='date_id')\n",
    "        target_pairs_df = pd.read_csv(self.target_pairs_path)\n",
    "        return train_df, test_df, train_labels_df, target_pairs_df\n",
    "\n",
    "class PreProcess:\n",
    "    \"\"\"PreProcess - regroupement des données par catégory\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_train_info(self, df):\n",
    "        \"\"\" Construction d'une data des entêtes de colonnes.\n",
    "        Args:\n",
    "            df (pd.DataFrame): Input dataframe d'entrainement.\n",
    "        Returns:\n",
    "            pd.DataFrame: Détail des informations de chaque colonne.\n",
    "        \"\"\"\n",
    "        df_names = df.columns\n",
    "        # Fonction pour nettoyer et split les noms\n",
    "        def clean_and_split(name):\n",
    "            name = name.replace(\"open_interest\", \"open interest\")\n",
    "            name = name.replace(\"settlement_price\", \"settlement price\")\n",
    "            name = name.replace(\"US_Stock\", \"US Stock\")\n",
    "            name = name.replace(\"adj_close\", \"Close\")\n",
    "            name = name.replace(\"adj_\", \"adjusted \")\n",
    "            name = name.replace(\"-\", \"_\")\n",
    "            return name.split(\"_\")\n",
    "\n",
    "        # Création du DataFrame d'infos\n",
    "        df_info = pd.DataFrame(\n",
    "            {\n",
    "            'Column': df_names,\n",
    "            'Split': [clean_and_split(name) for name in df_names]\n",
    "        })\n",
    "\n",
    "        df_info['Category'] = df_info['Split'].apply(lambda x: x[0])    \n",
    "        df_info['Ticker'] = df_info['Split'].apply(\n",
    "        lambda x: \"_\".join(x[1:-1]) if len(x) > 2 else x[-1] if len(x) == 2 else \"\"\n",
    "        )\n",
    "        df_info['Type'] = df_info['Split'].apply(lambda x: x[-1])\n",
    "\n",
    "        # Nettoyage final\n",
    "        df_info['Ticker'] = df_info.apply(\n",
    "            lambda row: row['Type'] if row['Ticker'] == \"\" else row['Ticker'], axis=1\n",
    "        )\n",
    "        df_info['Column_Id'] = df_info.index + 1\n",
    "\n",
    "        # Sélection des colonnes finales\n",
    "        df_info = df_info[['Column_Id', 'Column', 'Category', 'Ticker', 'Type']]    \n",
    "        return df_info\n",
    "    \n",
    "    def get_preprocess_data(self, df, cond):\n",
    "        # Fonction pour obtenir les données prétraitées en fonction de la condition\n",
    "        if cond.Column.size > 0:\n",
    "            return df[cond.Column.values[0]]\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def preprocess(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df_processed = df.copy()\n",
    "        df_processed = df_processed.drop(columns=['is_scored'], errors='ignore')\n",
    "        df_info = self.get_train_info(df_processed)\n",
    "\n",
    "\n",
    "        df_processed.reset_index(inplace=True)\n",
    "        # On renomme la date_id en date\n",
    "        df_processed = df_processed.rename({'date_id': 'date'}, axis='columns')\n",
    "        # Initialisation du DataFrame résultat\n",
    "        result = pd.DataFrame(columns=['date', 'id', 'close', 'open', 'high', 'low', 'volume', 'sprice', 'interest'])\n",
    "        \n",
    "        for  Category  in df_info.groupby('Category').groups.keys():\n",
    "            txtCategory=Category.replace(' ','_')\n",
    "            for label in df_info[(df_info.Category==Category)].groupby('Ticker').groups.keys():\n",
    "                temp_df = pd.DataFrame()\n",
    "                temp_df['date'] = df_processed['date']\n",
    "                temp_df['id'] = f'{txtCategory}_{label}'\n",
    "\n",
    "                if Category in ['FX','LME']:\n",
    "                    temp_df['close'] = df_processed[df_info[(df_info.Category==Category) & (df_info.Ticker==label)].Column.values[0]]\n",
    "                    temp_df['open'] = None\n",
    "                    temp_df['high'] = None\n",
    "                    temp_df['low'] = None\n",
    "                    temp_df['volume'] = None\n",
    "                    temp_df['sprice'] = None\n",
    "                    temp_df['interest'] = None\n",
    "                else:\n",
    "                    temp_df['close'] = self.get_preprocess_data(df_processed,df_info[(df_info.Category==Category) & (df_info.Ticker==label) & (df_info.Type.isin(['Close', 'adjusted close']))])\n",
    "                    temp_df['open'] = self.get_preprocess_data(df_processed,df_info[(df_info.Category==Category) & (df_info.Ticker==label) & (df_info.Type.isin(['Open','adjusted open']))])\n",
    "                    temp_df['high'] = self.get_preprocess_data(df_processed,df_info[(df_info.Category==Category) & (df_info.Ticker==label) & (df_info.Type.isin(['High','adjusted high']))])\n",
    "                    temp_df['low'] = self.get_preprocess_data(df_processed,df_info[(df_info.Category==Category) & (df_info.Ticker==label) & (df_info.Type.isin(['Low','adjusted low']))])\n",
    "                    temp_df['volume'] = self.get_preprocess_data(df_processed,df_info[(df_info.Category==Category) & (df_info.Ticker==label) & (df_info.Type.isin(['Volume', 'adjusted volume']))])\n",
    "                    temp_df['sprice'] = self.get_preprocess_data(df_processed,df_info[(df_info.Category==Category) & (df_info.Ticker==label) & (df_info.Type.isin(['settlement price','adjusted settlement price']))])\n",
    "                    temp_df['interest'] = self.get_preprocess_data(df_processed,df_info[(df_info.Category==Category) & (df_info.Ticker==label) & (df_info.Type.isin(['open interest','adjusted open interest']))])\n",
    "                result = pd.concat([result, temp_df], ignore_index=True)\n",
    "        \n",
    "        # Réinitialiser l'index\n",
    "        result = result.reset_index(drop=True)\n",
    "        \n",
    "        # Trier par date et id \n",
    "        result = result.sort_values(['date', 'id']).reset_index(drop=True)\n",
    "        \n",
    "        return result  \n",
    "    \n",
    "class FeatureEngineer:\n",
    "    \"\"\"Creation des fonctions \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def add_lag_features(self,\n",
    "        df: pd.DataFrame, \n",
    "        lags: List[int], \n",
    "        date_col: str = 'date'\n",
    "        ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Add lag features for specified columns and lags.\n",
    "        \"\"\"\n",
    "        df = df.sort_values(date_col)\n",
    "        cols = set(df.columns)\n",
    "        cols.remove('id')\n",
    "        cols.remove('date')\n",
    "        for col in cols:\n",
    "            for lag in lags:\n",
    "                df[f'{col}_lag{lag}'] = df.groupby('id')[col].shift(lag)\n",
    "        return df\n",
    "    \n",
    "    def add_rolling_features(self,\n",
    "        df: pd.DataFrame,\n",
    "        windows: List[int],\n",
    "        date_col: str = 'date') -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "        Add rolling mean and std features for specified columns and windows.\n",
    "        \"\"\"\n",
    "        df = df.sort_values(date_col)\n",
    "        cols = set(df.columns)\n",
    "        cols.remove('id')\n",
    "        cols.remove('date')\n",
    "        for col in cols:\n",
    "            for window in windows:\n",
    "                df[f'{col}_rollmean{window}'] = df.groupby('id')[col].transform(lambda x: x.rolling(window, min_periods=1).mean())\n",
    "                df[f'{col}_rollstd{window}'] = df.groupby('id')[col].transform(lambda x: x.rolling(window, min_periods=1).std())\n",
    "                df[f'{col}_rollmin{window}'] = df.groupby('id')[col].transform(lambda x: x.rolling(window, min_periods=1).min())\n",
    "                df[f'{col}_rollmax{window}'] = df.groupby('id')[col].transform(lambda x: x.rolling(window, min_periods=1).max())\n",
    "        return df\n",
    "    \n",
    "    def prepare_features(self, df: pd.DataFrame) ->pd.DataFrame:\n",
    "        \"\"\"Engineer features for training and testing data\"\"\"\n",
    "        try:\n",
    "            # add lag\n",
    "            df_result = df.copy()\n",
    "            df_result = self.add_lag_features(df_result, lags=[1, 2, 3, 5, 7])  \n",
    "            # Add rolling features\n",
    "            df_result = self.add_rolling_features(df_result, windows=[5, 10, 15])\n",
    "            # Handle missing values\n",
    "            df_result = df_result.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "\n",
    "            return df_result\n",
    "        except Exception as e:\n",
    "            print(f\"Feature preparation failed: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "86d46473",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTarget:\n",
    "    \"\"\"Class to handle target feature engineering\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def prepare_targets(self, train_labels_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Prepare target information from pairs DataFrame.\n",
    "        Args:\n",
    "            pairs (pd.DataFrame): DataFrame containing 'pair' column.   \n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with target information including price_1, price_2, and is_pair.\n",
    "        \"\"\"\n",
    "        target_cols = [col for col in train_labels_df.columns if col not in ['timestamp', 'id']]\n",
    "        target_values = train_labels_df[target_cols]\n",
    "        return target_values, target_cols\n",
    "    \n",
    "    def prepare_targets_info(self, pairs: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Prepare target information from pairs DataFrame.\n",
    "        Args:\n",
    "            pairs (pd.DataFrame): DataFrame containing 'pair' column.\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with target information including price_1, price_2, and is_pair.\n",
    "        \"\"\"\n",
    "\n",
    "        target_definitions = pairs[\"pair\"].str.split(\" - \", expand=True)\n",
    "        target_info = pairs.copy()\n",
    "\n",
    "        # Colonnes price_1 et price_2 (équivalent aux colonnes [,1] et [,2])\n",
    "        target_info[\"price_1\"] = target_definitions[0]\n",
    "        target_info[\"price_2\"] = target_definitions[1]\n",
    "\n",
    "        # is.pair = second élément non vide\n",
    "        target_info['is_pair'] = target_info['price_2'].apply(lambda x:x is not None)\n",
    "\n",
    "        # Retirer la colonne \"pair\"\n",
    "        target_info = target_info.drop(columns=[\"pair\"])\n",
    "        return target_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "892edd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "  \"\"\"LSTM model for time series prediction\"\"\"\n",
    "  def __init__(self,input_size,hidden_size=128,num_layers=2, dropout=0.2):\n",
    "    super().__init__()\n",
    "\n",
    "    # store parameters\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    # RNN Layer (notation: LSTM \\in RNN)\n",
    "    self.lstm = nn.LSTM(\n",
    "      input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "            )\n",
    "    \n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    # linear layer for output\n",
    "    self.fc = nn.Linear(hidden_size, 1)\n",
    "    self.name = \"LSTMModel\"\n",
    "    \n",
    "  def forward(self,x):\n",
    "    lstm_out, _ = self.lstm(x)\n",
    "    last_output = lstm_out[:, -1, :]\n",
    "    out = self.dropout(last_output)\n",
    "    out = self.fc(out)\n",
    "    return out\n",
    "  \n",
    "  def predict(self, dataloader, device=None, return_numpy=True):\n",
    "    self.eval()\n",
    "    if device is not None:\n",
    "        self.to(device)\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in dataloader:\n",
    "            if device is not None:\n",
    "                X_batch = X_batch.to(device)\n",
    "\n",
    "            output = self.forward(X_batch)\n",
    "            preds.append(output.cpu())\n",
    "\n",
    "    preds = torch.cat(preds, dim=0)\n",
    "\n",
    "    if return_numpy:\n",
    "        return preds.numpy()\n",
    "    return preds\n",
    "  \n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    \"\"\"GRU model for time series prediction\"\"\"\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.2):\n",
    "        super(GRUModel, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.name = \"GRUModel\"\n",
    "        \n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.gru(x)\n",
    "        last_output = gru_out[:, -1, :]\n",
    "        out = self.dropout(last_output)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "    def predict(self, dataloader, device=None, return_numpy=True):\n",
    "        self.eval()\n",
    "        if device is not None:\n",
    "            self.to(device)\n",
    "\n",
    "        preds = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch, _ in dataloader:\n",
    "                if device is not None:\n",
    "                    X_batch = X_batch.to(device)\n",
    "\n",
    "                output = self.forward(X_batch)\n",
    "                preds.append(output.cpu())\n",
    "\n",
    "        preds = torch.cat(preds, dim=0)\n",
    "\n",
    "        if return_numpy:\n",
    "            return preds.numpy()\n",
    "        return preds\n",
    "  \n",
    "    \n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for time series data\"\"\"\n",
    "    def __init__(self, X, y, sequence_length=10):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X) - self.sequence_length\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "           \n",
    "            self.X[idx:idx + self.sequence_length],\n",
    "            self.y[idx + self.sequence_length]\n",
    "\n",
    "        )\n",
    "    \n",
    "def prepare_data_train(X, y, sequence_length=10, batch_size=32):\n",
    "    \"\"\"Prepare data for training\"\"\"\n",
    "    # Scale features\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = TimeSeriesDataset(X_scaled, y, sequence_length)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return dataloader, scaler\n",
    "\n",
    "def prepare_test_data(X, y, scaler, sequence_length=10, batch_size=32):\n",
    "    X_scaled = scaler.transform(X)\n",
    "    display(X_scaled)\n",
    "    dataset = TimeSeriesDataset(X_scaled, y, sequence_length)\n",
    "    display(len(dataset))\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64708525",
   "metadata": {},
   "source": [
    "## Reprise de l'entrainement du modèle en ajoutant la prédiction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "29433caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y_pred, y_true):\n",
    "    return torch.mean(torch.abs(y_pred - y_true))\n",
    "\n",
    "def rmse(y_pred, y_true):\n",
    "    return torch.sqrt(torch.mean((y_pred - y_true) ** 2))\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for X, y in dataloader:\n",
    "        batch_X, batch_y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model(batch_X)\n",
    "        loss = criterion(y_pred.squeeze(), batch_y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, optimizer, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    y_preds, y_trues = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            batch_X, batch_y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(batch_X)\n",
    "            total_loss += criterion(y_pred.squeeze(), batch_y).item()\n",
    "            y_preds.append(y_pred)\n",
    "            y_trues.append(y)\n",
    "\n",
    "            \n",
    "    y_preds = torch.cat(y_preds)\n",
    "    y_trues = torch.cat(y_trues)\n",
    "\n",
    "    return (\n",
    "        total_loss / len(dataloader),\n",
    "        mae(y_preds, y_trues).item(),\n",
    "        rmse(y_preds, y_trues).item()\n",
    "    )\n",
    "\n",
    "class ParamTest:\n",
    "    def __init__(self):\n",
    "        self.hidden_size:int = 128\n",
    "        self.num_layers:int = 2\n",
    "        self.dropout:float = 0.2\n",
    "        self.learning_rate:float=0.001\n",
    "        self.batch_size:int = 32\n",
    "        self.sequence_length:int = 10\n",
    "        self.weight_decay:float = 0.2\n",
    "\n",
    "def train_model(params:ParamTest, model_type, X_train, y_train, X_val, y_val ,input_size, max_epochs, patience  ):\n",
    "# Prepare data\n",
    "    train_loader, scaler = prepare_data_train(\n",
    "                    X_train, y_train, \n",
    "                    params.sequence_length, params.batch_size\n",
    "                )\n",
    "    val_loader, _  = prepare_data_train(\n",
    "                    X_val, y_val, \n",
    "                    params.sequence_length, params.batch_size\n",
    "                )\n",
    "    \n",
    "    # Initialize model\n",
    "   \n",
    "    if model_type == 'LSTM':\n",
    "        model = LSTMModel(input_size, params.hidden_size,params.num_layers, params.dropout)\n",
    "    elif model_type == 'GRU':\n",
    "        model = GRUModel(input_size, params.hidden_size,params.num_layers, params.dropout)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type. Choose 'LSTM' or 'GRU'.\")\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params.learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=patience/2, factor=0.5)\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_maes = []\n",
    "    val_rmses = []\n",
    "    val_lrs = []\n",
    "    # Training loop\n",
    "    for epoch in range(max_epochs):\n",
    "       \n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "        train_losses.append(train_loss)\n",
    "        # Validation\n",
    "        if val_loader is not None:\n",
    "\n",
    "            val_loss, val_mae, val_rmse = evaluate(model, val_loader,optimizer,criterion )\n",
    "            \n",
    "            val_losses.append(val_loss)\n",
    "            val_maes.append(val_mae)\n",
    "            val_rmses.append(val_rmse)\n",
    "            current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "            val_lrs.append(current_lr)\n",
    "            # Learning rate scheduling\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "            \n",
    "            \n",
    "            if epoch % 2 == 0:\n",
    "                print(\n",
    "                    f\"Epoch {epoch+1:02d} | \"\n",
    "                    f\"LR: {current_lr:.1e} | \"\n",
    "                    f\"Train Loss: {train_loss:.4f} | \"\n",
    "                    f\"Val Loss: {val_loss:.4f} | \"\n",
    "                    f\"MAE: {val_mae:.4f} | \"\n",
    "                    f\"RMSE: {val_rmse:.4f}\"\n",
    "                )\n",
    "                \n",
    "        else:\n",
    "            if epoch % 2 == 0:\n",
    "                print(f\"Epoch {epoch}: Train Loss: {train_loss:.6f}\")    \n",
    "\n",
    "    return model, scaler\n",
    "\n",
    "# train target values\n",
    "def convert_to_colname(colname:str)-> str:\n",
    "    if colname is None:\n",
    "        return None\n",
    "    if colname.startswith(\"FX_\"):\n",
    "        return colname\n",
    "    colname = colname.replace(\"_adj\",\"\")\n",
    "    colname = colname.rsplit('_', 1)[0]\n",
    "    return colname\n",
    "\n",
    "def getnameprice(indexe_target:int):\n",
    "    target_name = target_info.loc[indexe_target,'target']\n",
    "    target_col1 = convert_to_colname(target_info.loc[indexe_target,'price_1'])\n",
    "    target_col2 = convert_to_colname(target_info.loc[indexe_target,'price_2'])\n",
    "    return  target_name, target_col1, target_col2\n",
    "\n",
    "def save_model(model_name, name_target, model, scaler):\n",
    "    model_dir = Path('../outputs') / 'models'\n",
    "    model_dir.mkdir(exist_ok=True)\n",
    "    model_path =  model_dir /  f\"{name_target}_{model_name}\"\n",
    "    torch.save({\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"input_size\": model.input_size,\n",
    "        \"hidden_size\": model.hidden_size,\n",
    "        \"num_layers\": model.num_layers\n",
    "    }, f\"{model_path}.ptk\")\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    assert isinstance(scaler, StandardScaler)\n",
    "    joblib.dump(scaler, f\"{model_path}_scaler.ptk\")\n",
    "    \n",
    "    \n",
    "\n",
    "def loadmodel(model_name, name_target, device=None):\n",
    "    model_dir = Path('../outputs') / 'models'\n",
    "    model_path =  model_dir /  f\"{name_target}_{model_name}\"\n",
    "    checkpoint = torch.load(f\"{model_path}.ptk\", map_location=device)\n",
    "\n",
    "    if model_name == \"GRUModel\":\n",
    "        model = GRUModel(\n",
    "        input_size=checkpoint[\"input_size\"],\n",
    "        hidden_size=checkpoint[\"hidden_size\"],\n",
    "        num_layers=checkpoint[\"num_layers\"]\n",
    "        )\n",
    "    else:\n",
    "        model = LSTMModel(\n",
    "        input_size=checkpoint[\"input_size\"],\n",
    "        hidden_size=checkpoint[\"hidden_size\"],\n",
    "        num_layers=checkpoint[\"num_layers\"]\n",
    "    )\n",
    "        \n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "    scaler = joblib.load(f\"{model_path}_scaler.ptk\")\n",
    "    \n",
    "    # Vérification explicite\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    assert isinstance(scaler, StandardScaler), f\"Scaler chargé = {type(scaler)}\"\n",
    "\n",
    "    if device is not None:\n",
    "        model.to(device)\n",
    "\n",
    "    return model, scaler\n",
    "    \n",
    "def train_features_build(target_col1,target_col2):\n",
    "# Filter for the two targets\n",
    "    if target_col2 is not None:\n",
    "        df_target2 = train_df_feature[train_df_feature['id'] == target_col2]\n",
    "        df_target1 = train_df_feature[train_df_feature['id'] == target_col1]\n",
    "        \n",
    "        # Concatenate the two DataFrames side by side (axis=1)\n",
    "        result = df_target2.merge(\n",
    "    df_target1,\n",
    "    on='date',\n",
    "    how='left',   # ou 'left'\n",
    "    suffixes=('_t2', '_t1'))\n",
    "        result = result.reset_index()\n",
    "        result = result.drop(['id_t2', 'date_t2','id_t1', 'date_t1', 'index', 'date'], axis=1, errors='ignore')\n",
    "        \n",
    "        return result\n",
    "    else:\n",
    "    \n",
    "        df_target1 = train_df_feature[train_df_feature['id'] == target_col1]\n",
    "        # Concatenate the two DataFrames side by side (axis=1)\n",
    "        result = df_target1\n",
    "        result = result.reset_index()\n",
    "        result = result.drop(['id', 'date', 'index'], axis=1, errors='ignore')\n",
    "        \n",
    "        return result\n",
    "                \n",
    "def my_train_models_for_target_save(name,\n",
    "                p_target_values,\n",
    "                target_col1:str,\n",
    "                target_col2:str,\n",
    "                model_type:str='LSTM',\n",
    "                num_epochs:int=50,\n",
    "                patience:int=30):\n",
    "    \n",
    "    y = p_target_values\n",
    "    mask = ~y.isna()\n",
    "    X_all = train_features_build(target_col1,target_col2)\n",
    "    \n",
    "    X_valid = X_all.loc[mask]\n",
    "    y_valid = y[mask]\n",
    "\n",
    "    # Simple train/validation split (last 20% for validation)\n",
    "    split_idx = int(len(X_valid) * 0.8)\n",
    "    X_train = X_valid[:split_idx]\n",
    "    y_train = y_valid[:split_idx]\n",
    "    X_val = X_valid[split_idx:]\n",
    "    y_val = y_valid[split_idx:]\n",
    "\n",
    "    params = ParamTest()\n",
    "    model, scaler  = \\\n",
    "        train_model(params, model_type, X_train, y_train.values, X_val, y_val.values, \n",
    "                    X_train.shape[1],num_epochs, patience)\n",
    "    save_model(model.name, name, model, scaler)\n",
    "\n",
    "loaderdata = LoaderData()\n",
    "featureTarget = FeatureTarget()\n",
    "featureEngineer = FeatureEngineer()\n",
    "preProcess = PreProcess()\n",
    "train_df, test_df, train_labels_df, target_pairs_df = loaderdata.load_data()\n",
    "train_df_process = preProcess.preprocess(train_df)\n",
    "train_df_feature = featureEngineer.prepare_features(train_df_process)\n",
    "target_info = featureTarget.prepare_targets_info(target_pairs_df)\n",
    "target_values,target_cols = featureTarget.prepare_targets(train_labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b4a484e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | LR: 1.0e-03 | Train Loss: 0.0340 | Val Loss: 0.0134 | MAE: 0.1038 | RMSE: 0.1166\n",
      "Epoch 03 | LR: 1.0e-03 | Train Loss: 0.0045 | Val Loss: 0.0023 | MAE: 0.0429 | RMSE: 0.0487\n",
      "Epoch 05 | LR: 1.0e-03 | Train Loss: 0.0022 | Val Loss: 0.0030 | MAE: 0.0446 | RMSE: 0.0557\n",
      "Epoch 07 | LR: 1.0e-03 | Train Loss: 0.0020 | Val Loss: 0.0011 | MAE: 0.0282 | RMSE: 0.0335\n",
      "Epoch 09 | LR: 1.0e-03 | Train Loss: 0.0012 | Val Loss: 0.0003 | MAE: 0.0144 | RMSE: 0.0182\n",
      "Epoch 11 | LR: 1.0e-03 | Train Loss: 0.0010 | Val Loss: 0.0003 | MAE: 0.0131 | RMSE: 0.0165\n",
      "Epoch 13 | LR: 1.0e-03 | Train Loss: 0.0007 | Val Loss: 0.0002 | MAE: 0.0110 | RMSE: 0.0147\n",
      "Epoch 15 | LR: 1.0e-03 | Train Loss: 0.0006 | Val Loss: 0.0002 | MAE: 0.0101 | RMSE: 0.0136\n",
      "Epoch 17 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0002 | MAE: 0.0115 | RMSE: 0.0153\n",
      "Epoch 19 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0002 | MAE: 0.0093 | RMSE: 0.0126\n",
      "Epoch 21 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0001 | MAE: 0.0092 | RMSE: 0.0126\n",
      "Epoch 23 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0002 | MAE: 0.0099 | RMSE: 0.0132\n",
      "Epoch 25 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0001 | MAE: 0.0082 | RMSE: 0.0116\n",
      "Epoch 27 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0002 | MAE: 0.0092 | RMSE: 0.0126\n",
      "Epoch 29 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0001 | MAE: 0.0086 | RMSE: 0.0119\n",
      "Epoch 31 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0001 | MAE: 0.0080 | RMSE: 0.0113\n",
      "Epoch 33 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0001 | MAE: 0.0081 | RMSE: 0.0116\n",
      "Epoch 35 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0090 | RMSE: 0.0121\n",
      "Epoch 37 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0001 | MAE: 0.0074 | RMSE: 0.0108\n",
      "Epoch 39 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0001 | MAE: 0.0090 | RMSE: 0.0124\n",
      "Epoch 41 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0001 | MAE: 0.0079 | RMSE: 0.0113\n",
      "Epoch 43 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0001 | MAE: 0.0077 | RMSE: 0.0110\n",
      "Epoch 45 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0001 | MAE: 0.0077 | RMSE: 0.0110\n",
      "Epoch 47 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0001 | MAE: 0.0078 | RMSE: 0.0111\n",
      "Epoch 49 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0001 | MAE: 0.0075 | RMSE: 0.0109\n",
      "Epoch 01 | LR: 1.0e-03 | Train Loss: 0.0037 | Val Loss: 0.0005 | MAE: 0.0167 | RMSE: 0.0221\n",
      "Epoch 03 | LR: 1.0e-03 | Train Loss: 0.0007 | Val Loss: 0.0003 | MAE: 0.0144 | RMSE: 0.0176\n",
      "Epoch 05 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0006 | MAE: 0.0200 | RMSE: 0.0254\n",
      "Epoch 07 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0001 | MAE: 0.0082 | RMSE: 0.0120\n",
      "Epoch 09 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0001 | MAE: 0.0076 | RMSE: 0.0109\n",
      "Epoch 11 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0001 | MAE: 0.0089 | RMSE: 0.0120\n",
      "Epoch 13 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0001 | MAE: 0.0087 | RMSE: 0.0119\n",
      "Epoch 15 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0001 | MAE: 0.0084 | RMSE: 0.0116\n",
      "Epoch 17 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0001 | MAE: 0.0082 | RMSE: 0.0115\n",
      "Epoch 19 | LR: 1.0e-03 | Train Loss: 0.0001 | Val Loss: 0.0001 | MAE: 0.0089 | RMSE: 0.0121\n",
      "Epoch 21 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0001 | MAE: 0.0086 | RMSE: 0.0119\n",
      "Epoch 23 | LR: 1.0e-03 | Train Loss: 0.0001 | Val Loss: 0.0001 | MAE: 0.0074 | RMSE: 0.0108\n",
      "Epoch 25 | LR: 1.0e-03 | Train Loss: 0.0001 | Val Loss: 0.0001 | MAE: 0.0079 | RMSE: 0.0112\n",
      "Epoch 27 | LR: 1.0e-03 | Train Loss: 0.0001 | Val Loss: 0.0001 | MAE: 0.0077 | RMSE: 0.0111\n",
      "Epoch 29 | LR: 1.0e-03 | Train Loss: 0.0001 | Val Loss: 0.0001 | MAE: 0.0079 | RMSE: 0.0113\n",
      "Epoch 31 | LR: 1.0e-03 | Train Loss: 0.0001 | Val Loss: 0.0001 | MAE: 0.0078 | RMSE: 0.0113\n",
      "Epoch 33 | LR: 1.0e-03 | Train Loss: 0.0001 | Val Loss: 0.0001 | MAE: 0.0084 | RMSE: 0.0115\n",
      "Epoch 35 | LR: 1.0e-03 | Train Loss: 0.0001 | Val Loss: 0.0001 | MAE: 0.0083 | RMSE: 0.0115\n",
      "Epoch 37 | LR: 1.0e-03 | Train Loss: 0.0001 | Val Loss: 0.0001 | MAE: 0.0082 | RMSE: 0.0114\n",
      "Epoch 39 | LR: 1.0e-03 | Train Loss: 0.0001 | Val Loss: 0.0001 | MAE: 0.0079 | RMSE: 0.0111\n",
      "Epoch 41 | LR: 5.0e-04 | Train Loss: 0.0001 | Val Loss: 0.0001 | MAE: 0.0075 | RMSE: 0.0109\n",
      "Epoch 43 | LR: 5.0e-04 | Train Loss: 0.0001 | Val Loss: 0.0001 | MAE: 0.0079 | RMSE: 0.0114\n",
      "Epoch 45 | LR: 5.0e-04 | Train Loss: 0.0001 | Val Loss: 0.0001 | MAE: 0.0080 | RMSE: 0.0115\n",
      "Epoch 47 | LR: 5.0e-04 | Train Loss: 0.0001 | Val Loss: 0.0001 | MAE: 0.0079 | RMSE: 0.0113\n",
      "Epoch 49 | LR: 5.0e-04 | Train Loss: 0.0001 | Val Loss: 0.0001 | MAE: 0.0079 | RMSE: 0.0112\n",
      "Epoch 01 | LR: 1.0e-03 | Train Loss: 0.0451 | Val Loss: 0.0051 | MAE: 0.0578 | RMSE: 0.0701\n",
      "Epoch 03 | LR: 1.0e-03 | Train Loss: 0.0078 | Val Loss: 0.0046 | MAE: 0.0583 | RMSE: 0.0686\n",
      "Epoch 05 | LR: 1.0e-03 | Train Loss: 0.0074 | Val Loss: 0.0045 | MAE: 0.0481 | RMSE: 0.0641\n",
      "Epoch 07 | LR: 1.0e-03 | Train Loss: 0.0027 | Val Loss: 0.0020 | MAE: 0.0355 | RMSE: 0.0447\n",
      "Epoch 09 | LR: 1.0e-03 | Train Loss: 0.0022 | Val Loss: 0.0008 | MAE: 0.0240 | RMSE: 0.0295\n",
      "Epoch 11 | LR: 1.0e-03 | Train Loss: 0.0014 | Val Loss: 0.0006 | MAE: 0.0196 | RMSE: 0.0245\n",
      "Epoch 13 | LR: 1.0e-03 | Train Loss: 0.0011 | Val Loss: 0.0003 | MAE: 0.0143 | RMSE: 0.0185\n",
      "Epoch 15 | LR: 1.0e-03 | Train Loss: 0.0008 | Val Loss: 0.0003 | MAE: 0.0147 | RMSE: 0.0189\n",
      "Epoch 17 | LR: 1.0e-03 | Train Loss: 0.0007 | Val Loss: 0.0004 | MAE: 0.0157 | RMSE: 0.0200\n",
      "Epoch 19 | LR: 1.0e-03 | Train Loss: 0.0006 | Val Loss: 0.0002 | MAE: 0.0122 | RMSE: 0.0161\n",
      "Epoch 21 | LR: 1.0e-03 | Train Loss: 0.0006 | Val Loss: 0.0002 | MAE: 0.0126 | RMSE: 0.0165\n",
      "Epoch 23 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0003 | MAE: 0.0129 | RMSE: 0.0169\n",
      "Epoch 25 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0002 | MAE: 0.0116 | RMSE: 0.0154\n",
      "Epoch 27 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0003 | MAE: 0.0137 | RMSE: 0.0176\n",
      "Epoch 29 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0003 | MAE: 0.0130 | RMSE: 0.0170\n",
      "Epoch 31 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0003 | MAE: 0.0132 | RMSE: 0.0173\n",
      "Epoch 33 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0003 | MAE: 0.0147 | RMSE: 0.0187\n",
      "Epoch 35 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0003 | MAE: 0.0135 | RMSE: 0.0175\n",
      "Epoch 37 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0117 | RMSE: 0.0155\n",
      "Epoch 39 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0126 | RMSE: 0.0166\n",
      "Epoch 41 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0117 | RMSE: 0.0156\n",
      "Epoch 43 | LR: 5.0e-04 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0115 | RMSE: 0.0153\n",
      "Epoch 45 | LR: 5.0e-04 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0111 | RMSE: 0.0148\n",
      "Epoch 47 | LR: 5.0e-04 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0111 | RMSE: 0.0149\n",
      "Epoch 49 | LR: 5.0e-04 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0114 | RMSE: 0.0152\n",
      "Epoch 01 | LR: 1.0e-03 | Train Loss: 0.0038 | Val Loss: 0.0010 | MAE: 0.0207 | RMSE: 0.0291\n",
      "Epoch 03 | LR: 1.0e-03 | Train Loss: 0.0011 | Val Loss: 0.0010 | MAE: 0.0239 | RMSE: 0.0302\n",
      "Epoch 05 | LR: 1.0e-03 | Train Loss: 0.0008 | Val Loss: 0.0004 | MAE: 0.0152 | RMSE: 0.0201\n",
      "Epoch 07 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0002 | MAE: 0.0110 | RMSE: 0.0147\n",
      "Epoch 09 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0110 | RMSE: 0.0148\n",
      "Epoch 11 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0113 | RMSE: 0.0151\n",
      "Epoch 13 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0115 | RMSE: 0.0154\n",
      "Epoch 15 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0123 | RMSE: 0.0163\n",
      "Epoch 17 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0122 | RMSE: 0.0161\n",
      "Epoch 19 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0003 | MAE: 0.0123 | RMSE: 0.0164\n",
      "Epoch 21 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0130 | RMSE: 0.0172\n",
      "Epoch 23 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0124 | RMSE: 0.0165\n",
      "Epoch 25 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0004 | MAE: 0.0147 | RMSE: 0.0191\n",
      "Epoch 27 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0003 | MAE: 0.0146 | RMSE: 0.0189\n",
      "Epoch 29 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0003 | MAE: 0.0124 | RMSE: 0.0164\n",
      "Epoch 31 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0003 | MAE: 0.0123 | RMSE: 0.0164\n",
      "Epoch 33 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0003 | MAE: 0.0140 | RMSE: 0.0182\n",
      "Epoch 35 | LR: 5.0e-04 | Train Loss: 0.0001 | Val Loss: 0.0003 | MAE: 0.0137 | RMSE: 0.0178\n",
      "Epoch 37 | LR: 5.0e-04 | Train Loss: 0.0001 | Val Loss: 0.0003 | MAE: 0.0127 | RMSE: 0.0169\n",
      "Early stopping at epoch 38\n"
     ]
    }
   ],
   "source": [
    "for idx in range(2):\n",
    "    tgt=target_cols[idx]\n",
    "    y = target_values[tgt]\n",
    "    name,pric1,pric2 =getnameprice(idx)\n",
    "    my_train_models_for_target_save(name,y,pric1,pric2, model_type='GRU')\n",
    "    my_train_models_for_target_save(name,y,pric1,pric2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301a652a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | LR: 1.0e-03 | Train Loss: 0.0613 | Val Loss: 0.0341 | MAE: 0.1610 | RMSE: 0.1820\n",
      "Epoch 03 | LR: 1.0e-03 | Train Loss: 0.0071 | Val Loss: 0.0043 | MAE: 0.0490 | RMSE: 0.0660\n",
      "Epoch 05 | LR: 1.0e-03 | Train Loss: 0.0030 | Val Loss: 0.0006 | MAE: 0.0200 | RMSE: 0.0252\n",
      "Epoch 07 | LR: 1.0e-03 | Train Loss: 0.0021 | Val Loss: 0.0006 | MAE: 0.0181 | RMSE: 0.0223\n",
      "Epoch 09 | LR: 1.0e-03 | Train Loss: 0.0018 | Val Loss: 0.0003 | MAE: 0.0143 | RMSE: 0.0181\n",
      "Epoch 11 | LR: 1.0e-03 | Train Loss: 0.0011 | Val Loss: 0.0002 | MAE: 0.0097 | RMSE: 0.0133\n",
      "Epoch 13 | LR: 1.0e-03 | Train Loss: 0.0010 | Val Loss: 0.0002 | MAE: 0.0101 | RMSE: 0.0137\n",
      "Epoch 15 | LR: 1.0e-03 | Train Loss: 0.0009 | Val Loss: 0.0004 | MAE: 0.0144 | RMSE: 0.0185\n",
      "Epoch 17 | LR: 1.0e-03 | Train Loss: 0.0008 | Val Loss: 0.0002 | MAE: 0.0106 | RMSE: 0.0141\n",
      "Epoch 19 | LR: 1.0e-03 | Train Loss: 0.0008 | Val Loss: 0.0002 | MAE: 0.0120 | RMSE: 0.0157\n",
      "Epoch 21 | LR: 1.0e-03 | Train Loss: 0.0007 | Val Loss: 0.0003 | MAE: 0.0129 | RMSE: 0.0162\n",
      "Epoch 23 | LR: 1.0e-03 | Train Loss: 0.0006 | Val Loss: 0.0002 | MAE: 0.0100 | RMSE: 0.0134\n",
      "Epoch 25 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0001 | MAE: 0.0083 | RMSE: 0.0117\n",
      "Epoch 27 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0002 | MAE: 0.0105 | RMSE: 0.0138\n",
      "Epoch 29 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0002 | MAE: 0.0106 | RMSE: 0.0141\n",
      "Epoch 31 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0002 | MAE: 0.0094 | RMSE: 0.0129\n",
      "Epoch 33 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0102 | RMSE: 0.0135\n",
      "Epoch 35 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0095 | RMSE: 0.0127\n",
      "Epoch 37 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0001 | MAE: 0.0087 | RMSE: 0.0121\n",
      "Epoch 39 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0001 | MAE: 0.0082 | RMSE: 0.0116\n",
      "Epoch 41 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0001 | MAE: 0.0087 | RMSE: 0.0119\n",
      "Epoch 43 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0001 | MAE: 0.0079 | RMSE: 0.0112\n",
      "Epoch 45 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0001 | MAE: 0.0084 | RMSE: 0.0118\n",
      "Epoch 47 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0001 | MAE: 0.0079 | RMSE: 0.0113\n",
      "Epoch 49 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0099 | RMSE: 0.0131\n",
      "Epoch 01 | LR: 1.0e-03 | Train Loss: 0.0044 | Val Loss: 0.0010 | MAE: 0.0285 | RMSE: 0.0328\n",
      "Epoch 03 | LR: 1.0e-03 | Train Loss: 0.0019 | Val Loss: 0.0035 | MAE: 0.0474 | RMSE: 0.0583\n",
      "Epoch 05 | LR: 1.0e-03 | Train Loss: 0.0007 | Val Loss: 0.0005 | MAE: 0.0160 | RMSE: 0.0217\n",
      "Epoch 07 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0099 | RMSE: 0.0130\n",
      "Epoch 09 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0098 | RMSE: 0.0129\n",
      "Epoch 11 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0094 | RMSE: 0.0126\n",
      "Epoch 13 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0098 | RMSE: 0.0132\n",
      "Epoch 15 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0001 | MAE: 0.0076 | RMSE: 0.0110\n",
      "Epoch 17 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0001 | MAE: 0.0074 | RMSE: 0.0108\n",
      "Epoch 19 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0001 | MAE: 0.0082 | RMSE: 0.0115\n",
      "Epoch 21 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0105 | RMSE: 0.0136\n",
      "Epoch 23 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0093 | RMSE: 0.0124\n",
      "Epoch 25 | LR: 1.0e-03 | Train Loss: 0.0001 | Val Loss: 0.0002 | MAE: 0.0097 | RMSE: 0.0129\n",
      "Epoch 27 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0109 | RMSE: 0.0143\n",
      "Epoch 29 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0001 | MAE: 0.0087 | RMSE: 0.0122\n",
      "Epoch 31 | LR: 1.0e-03 | Train Loss: 0.0001 | Val Loss: 0.0001 | MAE: 0.0080 | RMSE: 0.0114\n",
      "Epoch 33 | LR: 1.0e-03 | Train Loss: 0.0001 | Val Loss: 0.0001 | MAE: 0.0082 | RMSE: 0.0115\n",
      "Epoch 35 | LR: 5.0e-04 | Train Loss: 0.0001 | Val Loss: 0.0001 | MAE: 0.0079 | RMSE: 0.0113\n",
      "Epoch 37 | LR: 5.0e-04 | Train Loss: 0.0001 | Val Loss: 0.0001 | MAE: 0.0077 | RMSE: 0.0111\n",
      "Epoch 39 | LR: 5.0e-04 | Train Loss: 0.0001 | Val Loss: 0.0001 | MAE: 0.0076 | RMSE: 0.0109\n",
      "Epoch 41 | LR: 5.0e-04 | Train Loss: 0.0001 | Val Loss: 0.0001 | MAE: 0.0079 | RMSE: 0.0113\n",
      "Epoch 43 | LR: 5.0e-04 | Train Loss: 0.0001 | Val Loss: 0.0001 | MAE: 0.0076 | RMSE: 0.0110\n",
      "Epoch 45 | LR: 5.0e-04 | Train Loss: 0.0001 | Val Loss: 0.0001 | MAE: 0.0076 | RMSE: 0.0110\n",
      "Epoch 47 | LR: 5.0e-04 | Train Loss: 0.0001 | Val Loss: 0.0001 | MAE: 0.0078 | RMSE: 0.0113\n",
      "Early stopping at epoch 47\n",
      "Epoch 01 | LR: 1.0e-03 | Train Loss: 0.0373 | Val Loss: 0.0059 | MAE: 0.0631 | RMSE: 0.0764\n",
      "Epoch 03 | LR: 1.0e-03 | Train Loss: 0.0098 | Val Loss: 0.0076 | MAE: 0.0714 | RMSE: 0.0863\n",
      "Epoch 05 | LR: 1.0e-03 | Train Loss: 0.0041 | Val Loss: 0.0039 | MAE: 0.0492 | RMSE: 0.0602\n",
      "Epoch 07 | LR: 1.0e-03 | Train Loss: 0.0022 | Val Loss: 0.0007 | MAE: 0.0212 | RMSE: 0.0267\n",
      "Epoch 09 | LR: 1.0e-03 | Train Loss: 0.0015 | Val Loss: 0.0005 | MAE: 0.0168 | RMSE: 0.0213\n",
      "Epoch 11 | LR: 1.0e-03 | Train Loss: 0.0011 | Val Loss: 0.0004 | MAE: 0.0144 | RMSE: 0.0186\n",
      "Epoch 13 | LR: 1.0e-03 | Train Loss: 0.0009 | Val Loss: 0.0003 | MAE: 0.0138 | RMSE: 0.0179\n",
      "Epoch 15 | LR: 1.0e-03 | Train Loss: 0.0008 | Val Loss: 0.0003 | MAE: 0.0122 | RMSE: 0.0162\n",
      "Epoch 17 | LR: 1.0e-03 | Train Loss: 0.0007 | Val Loss: 0.0003 | MAE: 0.0124 | RMSE: 0.0163\n",
      "Epoch 19 | LR: 1.0e-03 | Train Loss: 0.0006 | Val Loss: 0.0003 | MAE: 0.0123 | RMSE: 0.0162\n",
      "Epoch 21 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0003 | MAE: 0.0123 | RMSE: 0.0162\n",
      "Epoch 23 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0003 | MAE: 0.0120 | RMSE: 0.0159\n",
      "Epoch 25 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0003 | MAE: 0.0125 | RMSE: 0.0164\n",
      "Epoch 27 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0003 | MAE: 0.0126 | RMSE: 0.0166\n",
      "Epoch 29 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0003 | MAE: 0.0139 | RMSE: 0.0180\n",
      "Epoch 31 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0002 | MAE: 0.0118 | RMSE: 0.0156\n",
      "Epoch 33 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0002 | MAE: 0.0119 | RMSE: 0.0157\n",
      "Epoch 35 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0125 | RMSE: 0.0164\n",
      "Epoch 37 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0123 | RMSE: 0.0162\n",
      "Epoch 39 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0115 | RMSE: 0.0153\n",
      "Epoch 41 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0125 | RMSE: 0.0164\n",
      "Epoch 43 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0121 | RMSE: 0.0160\n",
      "Epoch 45 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0111 | RMSE: 0.0149\n",
      "Epoch 47 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0120 | RMSE: 0.0158\n",
      "Epoch 49 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0123 | RMSE: 0.0162\n",
      "Epoch 01 | LR: 1.0e-03 | Train Loss: 0.0026 | Val Loss: 0.0005 | MAE: 0.0186 | RMSE: 0.0234\n",
      "Epoch 03 | LR: 1.0e-03 | Train Loss: 0.0008 | Val Loss: 0.0007 | MAE: 0.0211 | RMSE: 0.0258\n",
      "Epoch 05 | LR: 1.0e-03 | Train Loss: 0.0009 | Val Loss: 0.0009 | MAE: 0.0240 | RMSE: 0.0288\n",
      "Epoch 07 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0003 | MAE: 0.0115 | RMSE: 0.0154\n",
      "Epoch 09 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0120 | RMSE: 0.0158\n",
      "Epoch 11 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0131 | RMSE: 0.0171\n",
      "Epoch 13 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0137 | RMSE: 0.0178\n",
      "Epoch 15 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0145 | RMSE: 0.0186\n",
      "Epoch 17 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0004 | MAE: 0.0151 | RMSE: 0.0195\n",
      "Epoch 19 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0140 | RMSE: 0.0180\n",
      "Epoch 21 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0134 | RMSE: 0.0176\n",
      "Epoch 23 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0133 | RMSE: 0.0176\n",
      "Epoch 25 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0003 | MAE: 0.0126 | RMSE: 0.0166\n",
      "Epoch 27 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0004 | MAE: 0.0140 | RMSE: 0.0183\n",
      "Epoch 29 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0003 | MAE: 0.0127 | RMSE: 0.0168\n",
      "Epoch 31 | LR: 5.0e-04 | Train Loss: 0.0001 | Val Loss: 0.0003 | MAE: 0.0127 | RMSE: 0.0167\n",
      "Epoch 33 | LR: 5.0e-04 | Train Loss: 0.0001 | Val Loss: 0.0003 | MAE: 0.0135 | RMSE: 0.0176\n",
      "Epoch 35 | LR: 5.0e-04 | Train Loss: 0.0001 | Val Loss: 0.0003 | MAE: 0.0126 | RMSE: 0.0167\n",
      "Epoch 37 | LR: 5.0e-04 | Train Loss: 0.0001 | Val Loss: 0.0003 | MAE: 0.0129 | RMSE: 0.0170\n",
      "Early stopping at epoch 37\n",
      "Epoch 01 | LR: 1.0e-03 | Train Loss: 0.0232 | Val Loss: 0.0039 | MAE: 0.0509 | RMSE: 0.0625\n",
      "Epoch 03 | LR: 1.0e-03 | Train Loss: 0.0053 | Val Loss: 0.0026 | MAE: 0.0424 | RMSE: 0.0529\n",
      "Epoch 05 | LR: 1.0e-03 | Train Loss: 0.0024 | Val Loss: 0.0006 | MAE: 0.0192 | RMSE: 0.0240\n",
      "Epoch 07 | LR: 1.0e-03 | Train Loss: 0.0017 | Val Loss: 0.0003 | MAE: 0.0139 | RMSE: 0.0174\n",
      "Epoch 09 | LR: 1.0e-03 | Train Loss: 0.0012 | Val Loss: 0.0004 | MAE: 0.0166 | RMSE: 0.0211\n",
      "Epoch 11 | LR: 1.0e-03 | Train Loss: 0.0011 | Val Loss: 0.0005 | MAE: 0.0187 | RMSE: 0.0233\n",
      "Epoch 13 | LR: 1.0e-03 | Train Loss: 0.0010 | Val Loss: 0.0003 | MAE: 0.0151 | RMSE: 0.0189\n",
      "Epoch 15 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0002 | MAE: 0.0123 | RMSE: 0.0154\n",
      "Epoch 17 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0003 | MAE: 0.0129 | RMSE: 0.0162\n",
      "Epoch 19 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0003 | MAE: 0.0124 | RMSE: 0.0155\n",
      "Epoch 21 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0143 | RMSE: 0.0177\n",
      "Epoch 23 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0108 | RMSE: 0.0136\n",
      "Epoch 25 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0133 | RMSE: 0.0165\n",
      "Epoch 27 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0134 | RMSE: 0.0166\n",
      "Epoch 29 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0101 | RMSE: 0.0127\n",
      "Epoch 31 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0110 | RMSE: 0.0138\n",
      "Epoch 33 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0109 | RMSE: 0.0137\n",
      "Epoch 35 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0114 | RMSE: 0.0142\n",
      "Epoch 37 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0109 | RMSE: 0.0137\n",
      "Epoch 39 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0100 | RMSE: 0.0126\n",
      "Epoch 41 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0102 | RMSE: 0.0129\n",
      "Epoch 43 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0102 | RMSE: 0.0129\n",
      "Epoch 45 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0107 | RMSE: 0.0134\n",
      "Epoch 47 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0106 | RMSE: 0.0134\n",
      "Epoch 49 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0118 | RMSE: 0.0148\n",
      "Epoch 01 | LR: 1.0e-03 | Train Loss: 0.0020 | Val Loss: 0.0004 | MAE: 0.0158 | RMSE: 0.0198\n",
      "Epoch 03 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0002 | MAE: 0.0106 | RMSE: 0.0134\n",
      "Epoch 05 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0099 | RMSE: 0.0126\n",
      "Epoch 07 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0101 | RMSE: 0.0128\n",
      "Epoch 09 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0110 | RMSE: 0.0139\n",
      "Epoch 11 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0099 | RMSE: 0.0125\n",
      "Epoch 13 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0123 | RMSE: 0.0153\n",
      "Epoch 15 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0104 | RMSE: 0.0131\n",
      "Epoch 17 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0106 | RMSE: 0.0134\n",
      "Epoch 19 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0114 | RMSE: 0.0143\n",
      "Epoch 21 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0105 | RMSE: 0.0133\n",
      "Epoch 23 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0104 | RMSE: 0.0131\n",
      "Epoch 25 | LR: 5.0e-04 | Train Loss: 0.0001 | Val Loss: 0.0002 | MAE: 0.0102 | RMSE: 0.0130\n",
      "Epoch 27 | LR: 5.0e-04 | Train Loss: 0.0001 | Val Loss: 0.0002 | MAE: 0.0103 | RMSE: 0.0130\n",
      "Epoch 29 | LR: 5.0e-04 | Train Loss: 0.0001 | Val Loss: 0.0002 | MAE: 0.0105 | RMSE: 0.0132\n",
      "Epoch 31 | LR: 5.0e-04 | Train Loss: 0.0001 | Val Loss: 0.0002 | MAE: 0.0108 | RMSE: 0.0136\n",
      "Epoch 33 | LR: 5.0e-04 | Train Loss: 0.0001 | Val Loss: 0.0002 | MAE: 0.0111 | RMSE: 0.0140\n",
      "Early stopping at epoch 34\n",
      "Epoch 01 | LR: 1.0e-03 | Train Loss: 0.0295 | Val Loss: 0.0078 | MAE: 0.0757 | RMSE: 0.0889\n",
      "Epoch 03 | LR: 1.0e-03 | Train Loss: 0.0045 | Val Loss: 0.0017 | MAE: 0.0343 | RMSE: 0.0428\n",
      "Epoch 05 | LR: 1.0e-03 | Train Loss: 0.0029 | Val Loss: 0.0014 | MAE: 0.0311 | RMSE: 0.0371\n",
      "Epoch 07 | LR: 1.0e-03 | Train Loss: 0.0024 | Val Loss: 0.0014 | MAE: 0.0308 | RMSE: 0.0373\n",
      "Epoch 09 | LR: 1.0e-03 | Train Loss: 0.0017 | Val Loss: 0.0009 | MAE: 0.0262 | RMSE: 0.0313\n",
      "Epoch 11 | LR: 1.0e-03 | Train Loss: 0.0016 | Val Loss: 0.0009 | MAE: 0.0230 | RMSE: 0.0286\n",
      "Epoch 13 | LR: 1.0e-03 | Train Loss: 0.0011 | Val Loss: 0.0005 | MAE: 0.0185 | RMSE: 0.0231\n",
      "Epoch 15 | LR: 1.0e-03 | Train Loss: 0.0008 | Val Loss: 0.0003 | MAE: 0.0138 | RMSE: 0.0172\n",
      "Epoch 17 | LR: 1.0e-03 | Train Loss: 0.0006 | Val Loss: 0.0004 | MAE: 0.0171 | RMSE: 0.0206\n",
      "Epoch 19 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0002 | MAE: 0.0119 | RMSE: 0.0151\n",
      "Epoch 21 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0003 | MAE: 0.0148 | RMSE: 0.0185\n",
      "Epoch 23 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0003 | MAE: 0.0130 | RMSE: 0.0164\n",
      "Epoch 25 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0003 | MAE: 0.0131 | RMSE: 0.0164\n",
      "Epoch 27 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0007 | MAE: 0.0230 | RMSE: 0.0264\n",
      "Epoch 29 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0006 | MAE: 0.0206 | RMSE: 0.0240\n",
      "Epoch 31 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0150 | RMSE: 0.0187\n",
      "Epoch 33 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0110 | RMSE: 0.0140\n",
      "Epoch 35 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0141 | RMSE: 0.0175\n",
      "Epoch 37 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0002 | MAE: 0.0116 | RMSE: 0.0147\n",
      "Epoch 39 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0112 | RMSE: 0.0143\n",
      "Epoch 41 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0159 | RMSE: 0.0194\n",
      "Epoch 43 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0115 | RMSE: 0.0146\n",
      "Epoch 45 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0106 | RMSE: 0.0135\n",
      "Epoch 47 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0106 | RMSE: 0.0134\n",
      "Epoch 49 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0108 | RMSE: 0.0136\n",
      "Epoch 01 | LR: 1.0e-03 | Train Loss: 0.0018 | Val Loss: 0.0005 | MAE: 0.0177 | RMSE: 0.0218\n",
      "Epoch 03 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0002 | MAE: 0.0117 | RMSE: 0.0148\n",
      "Epoch 05 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0002 | MAE: 0.0113 | RMSE: 0.0143\n",
      "Epoch 07 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0108 | RMSE: 0.0137\n",
      "Epoch 09 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0114 | RMSE: 0.0145\n",
      "Epoch 11 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0137 | RMSE: 0.0171\n",
      "Epoch 13 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0005 | MAE: 0.0180 | RMSE: 0.0225\n",
      "Epoch 15 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0128 | RMSE: 0.0161\n",
      "Epoch 17 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0113 | RMSE: 0.0144\n",
      "Epoch 19 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0003 | MAE: 0.0138 | RMSE: 0.0173\n",
      "Epoch 21 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0003 | MAE: 0.0132 | RMSE: 0.0167\n",
      "Epoch 23 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0003 | MAE: 0.0131 | RMSE: 0.0165\n",
      "Epoch 25 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0121 | RMSE: 0.0153\n",
      "Epoch 27 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0122 | RMSE: 0.0154\n",
      "Epoch 29 | LR: 5.0e-04 | Train Loss: 0.0001 | Val Loss: 0.0002 | MAE: 0.0120 | RMSE: 0.0152\n",
      "Epoch 31 | LR: 5.0e-04 | Train Loss: 0.0001 | Val Loss: 0.0002 | MAE: 0.0118 | RMSE: 0.0150\n",
      "Epoch 33 | LR: 5.0e-04 | Train Loss: 0.0001 | Val Loss: 0.0003 | MAE: 0.0129 | RMSE: 0.0163\n",
      "Epoch 35 | LR: 5.0e-04 | Train Loss: 0.0001 | Val Loss: 0.0002 | MAE: 0.0121 | RMSE: 0.0153\n",
      "Early stopping at epoch 36\n",
      "Epoch 01 | LR: 1.0e-03 | Train Loss: 0.0678 | Val Loss: 0.0309 | MAE: 0.1601 | RMSE: 0.1753\n",
      "Epoch 03 | LR: 1.0e-03 | Train Loss: 0.0132 | Val Loss: 0.0230 | MAE: 0.1198 | RMSE: 0.1520\n",
      "Epoch 05 | LR: 1.0e-03 | Train Loss: 0.0123 | Val Loss: 0.0230 | MAE: 0.1349 | RMSE: 0.1519\n",
      "Epoch 07 | LR: 1.0e-03 | Train Loss: 0.0084 | Val Loss: 0.0259 | MAE: 0.1492 | RMSE: 0.1610\n",
      "Epoch 09 | LR: 1.0e-03 | Train Loss: 0.0071 | Val Loss: 0.0251 | MAE: 0.1355 | RMSE: 0.1593\n",
      "Epoch 11 | LR: 1.0e-03 | Train Loss: 0.0058 | Val Loss: 0.0137 | MAE: 0.1018 | RMSE: 0.1176\n",
      "Epoch 13 | LR: 1.0e-03 | Train Loss: 0.0043 | Val Loss: 0.0059 | MAE: 0.0668 | RMSE: 0.0778\n",
      "Epoch 15 | LR: 1.0e-03 | Train Loss: 0.0026 | Val Loss: 0.0018 | MAE: 0.0364 | RMSE: 0.0436\n",
      "Epoch 17 | LR: 1.0e-03 | Train Loss: 0.0011 | Val Loss: 0.0008 | MAE: 0.0220 | RMSE: 0.0282\n",
      "Epoch 19 | LR: 1.0e-03 | Train Loss: 0.0008 | Val Loss: 0.0007 | MAE: 0.0230 | RMSE: 0.0272\n",
      "Epoch 21 | LR: 1.0e-03 | Train Loss: 0.0007 | Val Loss: 0.0003 | MAE: 0.0130 | RMSE: 0.0170\n",
      "Epoch 23 | LR: 1.0e-03 | Train Loss: 0.0006 | Val Loss: 0.0003 | MAE: 0.0129 | RMSE: 0.0165\n",
      "Epoch 25 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0003 | MAE: 0.0122 | RMSE: 0.0158\n",
      "Epoch 27 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0003 | MAE: 0.0134 | RMSE: 0.0170\n",
      "Epoch 29 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0003 | MAE: 0.0129 | RMSE: 0.0168\n",
      "Epoch 31 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0003 | MAE: 0.0146 | RMSE: 0.0183\n",
      "Epoch 33 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0003 | MAE: 0.0134 | RMSE: 0.0174\n",
      "Epoch 35 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0131 | RMSE: 0.0167\n",
      "Epoch 37 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0123 | RMSE: 0.0161\n",
      "Epoch 39 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0146 | RMSE: 0.0183\n",
      "Epoch 41 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0127 | RMSE: 0.0166\n",
      "Epoch 43 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0129 | RMSE: 0.0165\n",
      "Epoch 45 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0120 | RMSE: 0.0157\n",
      "Epoch 47 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0126 | RMSE: 0.0162\n",
      "Epoch 49 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0121 | RMSE: 0.0158\n",
      "Epoch 01 | LR: 1.0e-03 | Train Loss: 0.0046 | Val Loss: 0.0009 | MAE: 0.0242 | RMSE: 0.0312\n",
      "Epoch 03 | LR: 1.0e-03 | Train Loss: 0.0013 | Val Loss: 0.0023 | MAE: 0.0405 | RMSE: 0.0473\n",
      "Epoch 05 | LR: 1.0e-03 | Train Loss: 0.0007 | Val Loss: 0.0006 | MAE: 0.0198 | RMSE: 0.0238\n",
      "Epoch 07 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0003 | MAE: 0.0137 | RMSE: 0.0179\n",
      "Epoch 09 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0005 | MAE: 0.0182 | RMSE: 0.0231\n",
      "Epoch 11 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0129 | RMSE: 0.0166\n",
      "Epoch 13 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0142 | RMSE: 0.0184\n",
      "Epoch 15 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0004 | MAE: 0.0153 | RMSE: 0.0191\n",
      "Epoch 17 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0005 | MAE: 0.0169 | RMSE: 0.0210\n",
      "Epoch 19 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0005 | MAE: 0.0175 | RMSE: 0.0219\n",
      "Epoch 21 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0005 | MAE: 0.0181 | RMSE: 0.0223\n",
      "Epoch 23 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0006 | MAE: 0.0205 | RMSE: 0.0252\n",
      "Epoch 25 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0005 | MAE: 0.0182 | RMSE: 0.0223\n",
      "Epoch 27 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0005 | MAE: 0.0173 | RMSE: 0.0222\n",
      "Epoch 29 | LR: 5.0e-04 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0123 | RMSE: 0.0160\n",
      "Epoch 31 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0122 | RMSE: 0.0157\n",
      "Epoch 33 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0120 | RMSE: 0.0156\n",
      "Epoch 35 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0122 | RMSE: 0.0158\n",
      "Epoch 37 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0003 | MAE: 0.0123 | RMSE: 0.0159\n",
      "Epoch 39 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0003 | MAE: 0.0124 | RMSE: 0.0161\n",
      "Epoch 41 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0003 | MAE: 0.0124 | RMSE: 0.0162\n",
      "Epoch 43 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0003 | MAE: 0.0130 | RMSE: 0.0168\n",
      "Epoch 45 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0003 | MAE: 0.0131 | RMSE: 0.0169\n",
      "Epoch 47 | LR: 2.5e-04 | Train Loss: 0.0002 | Val Loss: 0.0003 | MAE: 0.0130 | RMSE: 0.0169\n",
      "Epoch 49 | LR: 2.5e-04 | Train Loss: 0.0001 | Val Loss: 0.0003 | MAE: 0.0133 | RMSE: 0.0172\n",
      "Epoch 01 | LR: 1.0e-03 | Train Loss: 0.0309 | Val Loss: 0.0387 | MAE: 0.1641 | RMSE: 0.1956\n",
      "Epoch 03 | LR: 1.0e-03 | Train Loss: 0.0064 | Val Loss: 0.0080 | MAE: 0.0678 | RMSE: 0.0892\n",
      "Epoch 05 | LR: 1.0e-03 | Train Loss: 0.0046 | Val Loss: 0.0031 | MAE: 0.0432 | RMSE: 0.0540\n",
      "Epoch 07 | LR: 1.0e-03 | Train Loss: 0.0030 | Val Loss: 0.0009 | MAE: 0.0235 | RMSE: 0.0296\n",
      "Epoch 09 | LR: 1.0e-03 | Train Loss: 0.0023 | Val Loss: 0.0012 | MAE: 0.0288 | RMSE: 0.0360\n",
      "Epoch 11 | LR: 1.0e-03 | Train Loss: 0.0017 | Val Loss: 0.0020 | MAE: 0.0383 | RMSE: 0.0454\n",
      "Epoch 13 | LR: 1.0e-03 | Train Loss: 0.0021 | Val Loss: 0.0012 | MAE: 0.0291 | RMSE: 0.0356\n",
      "Epoch 15 | LR: 1.0e-03 | Train Loss: 0.0015 | Val Loss: 0.0007 | MAE: 0.0206 | RMSE: 0.0270\n",
      "Epoch 17 | LR: 1.0e-03 | Train Loss: 0.0010 | Val Loss: 0.0005 | MAE: 0.0177 | RMSE: 0.0226\n",
      "Epoch 19 | LR: 1.0e-03 | Train Loss: 0.0008 | Val Loss: 0.0005 | MAE: 0.0185 | RMSE: 0.0235\n",
      "Epoch 21 | LR: 1.0e-03 | Train Loss: 0.0008 | Val Loss: 0.0005 | MAE: 0.0180 | RMSE: 0.0230\n",
      "Epoch 23 | LR: 1.0e-03 | Train Loss: 0.0007 | Val Loss: 0.0005 | MAE: 0.0182 | RMSE: 0.0232\n",
      "Epoch 25 | LR: 1.0e-03 | Train Loss: 0.0007 | Val Loss: 0.0005 | MAE: 0.0184 | RMSE: 0.0235\n",
      "Epoch 27 | LR: 1.0e-03 | Train Loss: 0.0006 | Val Loss: 0.0005 | MAE: 0.0174 | RMSE: 0.0222\n",
      "Epoch 29 | LR: 1.0e-03 | Train Loss: 0.0006 | Val Loss: 0.0005 | MAE: 0.0175 | RMSE: 0.0223\n",
      "Epoch 31 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0004 | MAE: 0.0166 | RMSE: 0.0214\n",
      "Epoch 33 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0004 | MAE: 0.0162 | RMSE: 0.0209\n",
      "Epoch 35 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0004 | MAE: 0.0165 | RMSE: 0.0214\n",
      "Epoch 37 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0004 | MAE: 0.0165 | RMSE: 0.0213\n",
      "Epoch 39 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0004 | MAE: 0.0163 | RMSE: 0.0210\n",
      "Epoch 41 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0004 | MAE: 0.0161 | RMSE: 0.0208\n",
      "Epoch 43 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0004 | MAE: 0.0158 | RMSE: 0.0205\n",
      "Epoch 45 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0004 | MAE: 0.0160 | RMSE: 0.0207\n",
      "Epoch 47 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0004 | MAE: 0.0167 | RMSE: 0.0215\n",
      "Epoch 49 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0004 | MAE: 0.0160 | RMSE: 0.0207\n",
      "Epoch 01 | LR: 1.0e-03 | Train Loss: 0.0053 | Val Loss: 0.0009 | MAE: 0.0242 | RMSE: 0.0309\n",
      "Epoch 03 | LR: 1.0e-03 | Train Loss: 0.0009 | Val Loss: 0.0009 | MAE: 0.0246 | RMSE: 0.0301\n",
      "Epoch 05 | LR: 1.0e-03 | Train Loss: 0.0008 | Val Loss: 0.0007 | MAE: 0.0219 | RMSE: 0.0272\n",
      "Epoch 07 | LR: 1.0e-03 | Train Loss: 0.0006 | Val Loss: 0.0007 | MAE: 0.0218 | RMSE: 0.0271\n",
      "Epoch 09 | LR: 1.0e-03 | Train Loss: 0.0006 | Val Loss: 0.0006 | MAE: 0.0191 | RMSE: 0.0244\n",
      "Epoch 11 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0008 | MAE: 0.0226 | RMSE: 0.0278\n",
      "Epoch 13 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0005 | MAE: 0.0168 | RMSE: 0.0216\n",
      "Epoch 15 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0005 | MAE: 0.0181 | RMSE: 0.0230\n",
      "Epoch 17 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0006 | MAE: 0.0183 | RMSE: 0.0234\n",
      "Epoch 19 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0005 | MAE: 0.0175 | RMSE: 0.0225\n",
      "Epoch 21 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0005 | MAE: 0.0167 | RMSE: 0.0216\n",
      "Epoch 23 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0004 | MAE: 0.0164 | RMSE: 0.0212\n",
      "Epoch 25 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0006 | MAE: 0.0196 | RMSE: 0.0248\n",
      "Epoch 27 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0007 | MAE: 0.0204 | RMSE: 0.0259\n",
      "Epoch 29 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0005 | MAE: 0.0172 | RMSE: 0.0221\n",
      "Epoch 31 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0005 | MAE: 0.0175 | RMSE: 0.0225\n",
      "Epoch 33 | LR: 5.0e-04 | Train Loss: 0.0003 | Val Loss: 0.0005 | MAE: 0.0163 | RMSE: 0.0211\n",
      "Epoch 35 | LR: 5.0e-04 | Train Loss: 0.0003 | Val Loss: 0.0005 | MAE: 0.0166 | RMSE: 0.0215\n",
      "Epoch 37 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0005 | MAE: 0.0170 | RMSE: 0.0218\n",
      "Epoch 39 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0005 | MAE: 0.0176 | RMSE: 0.0226\n",
      "Epoch 41 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0005 | MAE: 0.0173 | RMSE: 0.0222\n",
      "Epoch 43 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0005 | MAE: 0.0170 | RMSE: 0.0219\n",
      "Epoch 45 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0006 | MAE: 0.0184 | RMSE: 0.0237\n",
      "Early stopping at epoch 45\n",
      "Epoch 01 | LR: 1.0e-03 | Train Loss: 0.0216 | Val Loss: 0.0019 | MAE: 0.0294 | RMSE: 0.0374\n",
      "Epoch 03 | LR: 1.0e-03 | Train Loss: 0.0038 | Val Loss: 0.0013 | MAE: 0.0252 | RMSE: 0.0319\n",
      "Epoch 05 | LR: 1.0e-03 | Train Loss: 0.0019 | Val Loss: 0.0004 | MAE: 0.0174 | RMSE: 0.0221\n",
      "Epoch 07 | LR: 1.0e-03 | Train Loss: 0.0017 | Val Loss: 0.0004 | MAE: 0.0163 | RMSE: 0.0205\n",
      "Epoch 09 | LR: 1.0e-03 | Train Loss: 0.0009 | Val Loss: 0.0003 | MAE: 0.0142 | RMSE: 0.0178\n",
      "Epoch 11 | LR: 1.0e-03 | Train Loss: 0.0008 | Val Loss: 0.0003 | MAE: 0.0128 | RMSE: 0.0163\n",
      "Epoch 13 | LR: 1.0e-03 | Train Loss: 0.0007 | Val Loss: 0.0003 | MAE: 0.0148 | RMSE: 0.0184\n",
      "Epoch 15 | LR: 1.0e-03 | Train Loss: 0.0006 | Val Loss: 0.0003 | MAE: 0.0148 | RMSE: 0.0185\n",
      "Epoch 17 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0003 | MAE: 0.0142 | RMSE: 0.0178\n",
      "Epoch 19 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0004 | MAE: 0.0176 | RMSE: 0.0216\n",
      "Epoch 21 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0007 | MAE: 0.0234 | RMSE: 0.0281\n",
      "Epoch 23 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0006 | MAE: 0.0219 | RMSE: 0.0261\n",
      "Epoch 25 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0007 | MAE: 0.0240 | RMSE: 0.0283\n",
      "Epoch 27 | LR: 5.0e-04 | Train Loss: 0.0005 | Val Loss: 0.0004 | MAE: 0.0166 | RMSE: 0.0206\n",
      "Epoch 29 | LR: 5.0e-04 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0128 | RMSE: 0.0160\n",
      "Epoch 31 | LR: 5.0e-04 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0112 | RMSE: 0.0145\n",
      "Epoch 33 | LR: 5.0e-04 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0110 | RMSE: 0.0141\n",
      "Epoch 35 | LR: 5.0e-04 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0109 | RMSE: 0.0139\n",
      "Epoch 37 | LR: 5.0e-04 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0107 | RMSE: 0.0138\n",
      "Epoch 39 | LR: 5.0e-04 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0109 | RMSE: 0.0140\n",
      "Epoch 41 | LR: 5.0e-04 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0112 | RMSE: 0.0142\n",
      "Epoch 43 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0109 | RMSE: 0.0140\n",
      "Epoch 45 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0108 | RMSE: 0.0138\n",
      "Epoch 47 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0108 | RMSE: 0.0138\n",
      "Epoch 49 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0109 | RMSE: 0.0141\n",
      "Epoch 01 | LR: 1.0e-03 | Train Loss: 0.0024 | Val Loss: 0.0004 | MAE: 0.0149 | RMSE: 0.0189\n",
      "Epoch 03 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0002 | MAE: 0.0113 | RMSE: 0.0144\n",
      "Epoch 05 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0002 | MAE: 0.0117 | RMSE: 0.0150\n",
      "Epoch 07 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0121 | RMSE: 0.0154\n",
      "Epoch 09 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0003 | MAE: 0.0148 | RMSE: 0.0186\n",
      "Epoch 11 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0131 | RMSE: 0.0165\n",
      "Epoch 13 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0004 | MAE: 0.0156 | RMSE: 0.0196\n",
      "Epoch 15 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0004 | MAE: 0.0161 | RMSE: 0.0201\n",
      "Epoch 17 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0004 | MAE: 0.0160 | RMSE: 0.0199\n",
      "Epoch 19 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0004 | MAE: 0.0173 | RMSE: 0.0215\n",
      "Epoch 21 | LR: 5.0e-04 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0116 | RMSE: 0.0148\n",
      "Epoch 23 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0003 | MAE: 0.0131 | RMSE: 0.0167\n",
      "Epoch 25 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0117 | RMSE: 0.0150\n",
      "Epoch 27 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0121 | RMSE: 0.0156\n",
      "Epoch 29 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0003 | MAE: 0.0127 | RMSE: 0.0163\n",
      "Epoch 31 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0125 | RMSE: 0.0160\n",
      "Early stopping at epoch 32\n",
      "Epoch 01 | LR: 1.0e-03 | Train Loss: 0.0510 | Val Loss: 0.0166 | MAE: 0.1132 | RMSE: 0.1312\n",
      "Epoch 03 | LR: 1.0e-03 | Train Loss: 0.0092 | Val Loss: 0.0019 | MAE: 0.0336 | RMSE: 0.0428\n",
      "Epoch 05 | LR: 1.0e-03 | Train Loss: 0.0048 | Val Loss: 0.0025 | MAE: 0.0404 | RMSE: 0.0500\n",
      "Epoch 07 | LR: 1.0e-03 | Train Loss: 0.0032 | Val Loss: 0.0026 | MAE: 0.0441 | RMSE: 0.0518\n",
      "Epoch 09 | LR: 1.0e-03 | Train Loss: 0.0031 | Val Loss: 0.0011 | MAE: 0.0273 | RMSE: 0.0338\n",
      "Epoch 11 | LR: 1.0e-03 | Train Loss: 0.0017 | Val Loss: 0.0015 | MAE: 0.0315 | RMSE: 0.0387\n",
      "Epoch 13 | LR: 1.0e-03 | Train Loss: 0.0015 | Val Loss: 0.0010 | MAE: 0.0259 | RMSE: 0.0323\n",
      "Epoch 15 | LR: 1.0e-03 | Train Loss: 0.0011 | Val Loss: 0.0011 | MAE: 0.0286 | RMSE: 0.0333\n",
      "Epoch 17 | LR: 1.0e-03 | Train Loss: 0.0011 | Val Loss: 0.0004 | MAE: 0.0157 | RMSE: 0.0202\n",
      "Epoch 19 | LR: 1.0e-03 | Train Loss: 0.0009 | Val Loss: 0.0004 | MAE: 0.0165 | RMSE: 0.0210\n",
      "Epoch 21 | LR: 1.0e-03 | Train Loss: 0.0008 | Val Loss: 0.0004 | MAE: 0.0148 | RMSE: 0.0191\n",
      "Epoch 23 | LR: 1.0e-03 | Train Loss: 0.0006 | Val Loss: 0.0004 | MAE: 0.0153 | RMSE: 0.0196\n",
      "Epoch 25 | LR: 1.0e-03 | Train Loss: 0.0006 | Val Loss: 0.0004 | MAE: 0.0149 | RMSE: 0.0192\n",
      "Epoch 27 | LR: 1.0e-03 | Train Loss: 0.0006 | Val Loss: 0.0004 | MAE: 0.0151 | RMSE: 0.0195\n",
      "Epoch 29 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0005 | MAE: 0.0174 | RMSE: 0.0219\n",
      "Epoch 31 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0003 | MAE: 0.0144 | RMSE: 0.0186\n",
      "Epoch 33 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0003 | MAE: 0.0141 | RMSE: 0.0182\n",
      "Epoch 35 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0003 | MAE: 0.0136 | RMSE: 0.0177\n",
      "Epoch 37 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0003 | MAE: 0.0148 | RMSE: 0.0190\n",
      "Epoch 39 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0004 | MAE: 0.0153 | RMSE: 0.0196\n",
      "Epoch 41 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0004 | MAE: 0.0158 | RMSE: 0.0202\n",
      "Epoch 43 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0005 | MAE: 0.0185 | RMSE: 0.0232\n",
      "Epoch 45 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0004 | MAE: 0.0155 | RMSE: 0.0200\n",
      "Epoch 47 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0003 | MAE: 0.0149 | RMSE: 0.0191\n",
      "Epoch 49 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0004 | MAE: 0.0148 | RMSE: 0.0190\n",
      "Epoch 01 | LR: 1.0e-03 | Train Loss: 0.0048 | Val Loss: 0.0012 | MAE: 0.0275 | RMSE: 0.0341\n",
      "Epoch 03 | LR: 1.0e-03 | Train Loss: 0.0009 | Val Loss: 0.0006 | MAE: 0.0195 | RMSE: 0.0247\n",
      "Epoch 05 | LR: 1.0e-03 | Train Loss: 0.0006 | Val Loss: 0.0006 | MAE: 0.0200 | RMSE: 0.0248\n",
      "Epoch 07 | LR: 1.0e-03 | Train Loss: 0.0006 | Val Loss: 0.0005 | MAE: 0.0188 | RMSE: 0.0234\n",
      "Epoch 09 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0003 | MAE: 0.0144 | RMSE: 0.0186\n",
      "Epoch 11 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0005 | MAE: 0.0175 | RMSE: 0.0222\n",
      "Epoch 13 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0005 | MAE: 0.0167 | RMSE: 0.0212\n",
      "Epoch 15 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0004 | MAE: 0.0155 | RMSE: 0.0201\n",
      "Epoch 17 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0006 | MAE: 0.0195 | RMSE: 0.0243\n",
      "Epoch 19 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0007 | MAE: 0.0206 | RMSE: 0.0256\n",
      "Epoch 21 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0004 | MAE: 0.0164 | RMSE: 0.0209\n",
      "Epoch 23 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0005 | MAE: 0.0167 | RMSE: 0.0214\n",
      "Epoch 25 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0004 | MAE: 0.0160 | RMSE: 0.0208\n",
      "Epoch 27 | LR: 5.0e-04 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0138 | RMSE: 0.0179\n",
      "Epoch 29 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0003 | MAE: 0.0140 | RMSE: 0.0182\n",
      "Epoch 31 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0003 | MAE: 0.0140 | RMSE: 0.0183\n",
      "Epoch 33 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0003 | MAE: 0.0144 | RMSE: 0.0186\n",
      "Epoch 35 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0004 | MAE: 0.0145 | RMSE: 0.0189\n",
      "Epoch 37 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0004 | MAE: 0.0152 | RMSE: 0.0195\n",
      "Epoch 39 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0004 | MAE: 0.0150 | RMSE: 0.0194\n",
      "Epoch 41 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0004 | MAE: 0.0161 | RMSE: 0.0206\n",
      "Epoch 43 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0004 | MAE: 0.0152 | RMSE: 0.0197\n",
      "Epoch 45 | LR: 2.5e-04 | Train Loss: 0.0002 | Val Loss: 0.0004 | MAE: 0.0149 | RMSE: 0.0193\n",
      "Epoch 47 | LR: 2.5e-04 | Train Loss: 0.0001 | Val Loss: 0.0004 | MAE: 0.0148 | RMSE: 0.0192\n",
      "Epoch 49 | LR: 2.5e-04 | Train Loss: 0.0001 | Val Loss: 0.0004 | MAE: 0.0151 | RMSE: 0.0196\n",
      "Epoch 01 | LR: 1.0e-03 | Train Loss: 0.0358 | Val Loss: 0.0153 | MAE: 0.1049 | RMSE: 0.1244\n",
      "Epoch 03 | LR: 1.0e-03 | Train Loss: 0.0130 | Val Loss: 0.0196 | MAE: 0.1154 | RMSE: 0.1394\n",
      "Epoch 05 | LR: 1.0e-03 | Train Loss: 0.0106 | Val Loss: 0.0297 | MAE: 0.1412 | RMSE: 0.1757\n",
      "Epoch 07 | LR: 1.0e-03 | Train Loss: 0.0078 | Val Loss: 0.0051 | MAE: 0.0614 | RMSE: 0.0722\n",
      "Epoch 09 | LR: 1.0e-03 | Train Loss: 0.0047 | Val Loss: 0.0019 | MAE: 0.0377 | RMSE: 0.0443\n",
      "Epoch 11 | LR: 1.0e-03 | Train Loss: 0.0020 | Val Loss: 0.0012 | MAE: 0.0268 | RMSE: 0.0332\n",
      "Epoch 13 | LR: 1.0e-03 | Train Loss: 0.0014 | Val Loss: 0.0004 | MAE: 0.0157 | RMSE: 0.0205\n",
      "Epoch 15 | LR: 1.0e-03 | Train Loss: 0.0009 | Val Loss: 0.0003 | MAE: 0.0135 | RMSE: 0.0177\n",
      "Epoch 17 | LR: 1.0e-03 | Train Loss: 0.0007 | Val Loss: 0.0004 | MAE: 0.0155 | RMSE: 0.0200\n",
      "Epoch 19 | LR: 1.0e-03 | Train Loss: 0.0006 | Val Loss: 0.0004 | MAE: 0.0146 | RMSE: 0.0187\n",
      "Epoch 21 | LR: 1.0e-03 | Train Loss: 0.0006 | Val Loss: 0.0004 | MAE: 0.0156 | RMSE: 0.0201\n",
      "Epoch 23 | LR: 1.0e-03 | Train Loss: 0.0006 | Val Loss: 0.0004 | MAE: 0.0160 | RMSE: 0.0204\n",
      "Epoch 25 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0003 | MAE: 0.0131 | RMSE: 0.0175\n",
      "Epoch 27 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0003 | MAE: 0.0136 | RMSE: 0.0179\n",
      "Epoch 29 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0003 | MAE: 0.0127 | RMSE: 0.0170\n",
      "Epoch 31 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0123 | RMSE: 0.0165\n",
      "Epoch 33 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0123 | RMSE: 0.0164\n",
      "Epoch 35 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0123 | RMSE: 0.0165\n",
      "Epoch 37 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0122 | RMSE: 0.0163\n",
      "Epoch 39 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0123 | RMSE: 0.0163\n",
      "Epoch 41 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0141 | RMSE: 0.0182\n",
      "Epoch 43 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0118 | RMSE: 0.0158\n",
      "Epoch 45 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0003 | MAE: 0.0122 | RMSE: 0.0163\n",
      "Epoch 47 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0003 | MAE: 0.0118 | RMSE: 0.0159\n",
      "Epoch 49 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0003 | MAE: 0.0119 | RMSE: 0.0160\n",
      "Epoch 01 | LR: 1.0e-03 | Train Loss: 0.0053 | Val Loss: 0.0032 | MAE: 0.0492 | RMSE: 0.0585\n",
      "Epoch 03 | LR: 1.0e-03 | Train Loss: 0.0022 | Val Loss: 0.0026 | MAE: 0.0384 | RMSE: 0.0499\n",
      "Epoch 05 | LR: 1.0e-03 | Train Loss: 0.0020 | Val Loss: 0.0011 | MAE: 0.0280 | RMSE: 0.0332\n",
      "Epoch 07 | LR: 1.0e-03 | Train Loss: 0.0007 | Val Loss: 0.0005 | MAE: 0.0169 | RMSE: 0.0218\n",
      "Epoch 09 | LR: 1.0e-03 | Train Loss: 0.0004 | Val Loss: 0.0003 | MAE: 0.0120 | RMSE: 0.0160\n",
      "Epoch 11 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0003 | MAE: 0.0124 | RMSE: 0.0164\n",
      "Epoch 13 | LR: 1.0e-03 | Train Loss: 0.0003 | Val Loss: 0.0002 | MAE: 0.0113 | RMSE: 0.0153\n",
      "Epoch 15 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0114 | RMSE: 0.0154\n",
      "Epoch 17 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0112 | RMSE: 0.0152\n",
      "Epoch 19 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0117 | RMSE: 0.0157\n",
      "Epoch 21 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0112 | RMSE: 0.0153\n",
      "Epoch 23 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0117 | RMSE: 0.0157\n",
      "Epoch 25 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0113 | RMSE: 0.0153\n",
      "Epoch 27 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0118 | RMSE: 0.0158\n",
      "Epoch 29 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0115 | RMSE: 0.0155\n",
      "Epoch 31 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0003 | MAE: 0.0117 | RMSE: 0.0157\n",
      "Epoch 33 | LR: 1.0e-03 | Train Loss: 0.0002 | Val Loss: 0.0002 | MAE: 0.0117 | RMSE: 0.0157\n",
      "Epoch 35 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0003 | MAE: 0.0117 | RMSE: 0.0158\n",
      "Epoch 37 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0003 | MAE: 0.0119 | RMSE: 0.0160\n",
      "Epoch 39 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0003 | MAE: 0.0118 | RMSE: 0.0158\n",
      "Epoch 41 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0003 | MAE: 0.0119 | RMSE: 0.0159\n",
      "Epoch 43 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0003 | MAE: 0.0122 | RMSE: 0.0163\n",
      "Epoch 45 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0003 | MAE: 0.0119 | RMSE: 0.0160\n",
      "Epoch 47 | LR: 5.0e-04 | Train Loss: 0.0002 | Val Loss: 0.0003 | MAE: 0.0119 | RMSE: 0.0160\n",
      "Early stopping at epoch 47\n",
      "Epoch 01 | LR: 1.0e-03 | Train Loss: 0.0244 | Val Loss: 0.0033 | MAE: 0.0438 | RMSE: 0.0545\n",
      "Epoch 03 | LR: 1.0e-03 | Train Loss: 0.0032 | Val Loss: 0.0010 | MAE: 0.0249 | RMSE: 0.0311\n",
      "Epoch 05 | LR: 1.0e-03 | Train Loss: 0.0021 | Val Loss: 0.0008 | MAE: 0.0234 | RMSE: 0.0284\n",
      "Epoch 07 | LR: 1.0e-03 | Train Loss: 0.0014 | Val Loss: 0.0005 | MAE: 0.0170 | RMSE: 0.0213\n",
      "Epoch 09 | LR: 1.0e-03 | Train Loss: 0.0011 | Val Loss: 0.0005 | MAE: 0.0165 | RMSE: 0.0210\n",
      "Epoch 11 | LR: 1.0e-03 | Train Loss: 0.0010 | Val Loss: 0.0003 | MAE: 0.0133 | RMSE: 0.0170\n",
      "Epoch 13 | LR: 1.0e-03 | Train Loss: 0.0006 | Val Loss: 0.0002 | MAE: 0.0124 | RMSE: 0.0158\n",
      "Epoch 15 | LR: 1.0e-03 | Train Loss: 0.0005 | Val Loss: 0.0002 | MAE: 0.0118 | RMSE: 0.0151\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(target_cols)):\n",
    "    tgt=target_cols[idx]\n",
    "    y = target_values[tgt]\n",
    "    name,pric1,pric2 =getnameprice(idx)\n",
    "    my_train_models_for_target_save(name,y,pric1,pric2, model_type='GRU')\n",
    "    my_train_models_for_target_save(name,y,pric1,pric2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72a88da",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "f583230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "competition_data_dir = Path(\"../data\")\n",
    "def generate_data_batches():\n",
    "        test = pl.read_csv(competition_data_dir / 'test.csv')\n",
    "        test_df = pd.read_csv(competition_data_dir / 'test.csv', index_col='date_id')\n",
    "        preProcess = PreProcess()\n",
    "        \n",
    "        featureEngineer = FeatureEngineer()     \n",
    "        test_df_process = preProcess.preprocess(test_df)\n",
    "        test_df_feature = featureEngineer.prepare_features(test_df_process)\n",
    "        label_lag_dir = competition_data_dir / 'lagged_test_labels'\n",
    "        label_lags_1 = pl.read_csv(label_lag_dir / 'test_labels_lag_1.csv')\n",
    "        label_lags_2 = pl.read_csv(label_lag_dir / 'test_labels_lag_2.csv')\n",
    "        label_lags_3 = pl.read_csv(label_lag_dir / 'test_labels_lag_3.csv')\n",
    "        label_lags_4 = pl.read_csv(label_lag_dir / 'test_labels_lag_4.csv')\n",
    "\n",
    "        date_ids = test['date_id'].unique(maintain_order=True).to_list()\n",
    "        for date_id in date_ids:\n",
    "            test_batch = test_df_feature[test_df_feature['date'] == date_id]\n",
    "            # test_batch = test.filter(pl.col('date_id') == date_id)\n",
    "            label_lags_1_batch = label_lags_1.filter(pl.col('label_date_id') == date_id)\n",
    "            label_lags_2_batch = label_lags_2.filter(pl.col('label_date_id') == date_id)\n",
    "            label_lags_3_batch = label_lags_3.filter(pl.col('label_date_id') == date_id)\n",
    "            label_lags_4_batch = label_lags_4.filter(pl.col('label_date_id') == date_id)\n",
    "\n",
    "            yield (\n",
    "                (test_batch, label_lags_1_batch, label_lags_2_batch, label_lags_3_batch, label_lags_4_batch),\n",
    "                date_id,\n",
    "            )\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "d727d380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  chargement des models\n",
    "models_gru = {}\n",
    "scalers_gru = {}\n",
    "scalers_lstm = {}\n",
    "models_lstm = {}\n",
    "for idx in range(2):\n",
    "     tgt=target_cols[idx]\n",
    "     models_gru[tgt], scalers_gru[tgt] = loadmodel('GRUModel',tgt)\n",
    "     models_lstm[tgt], scalers_lstm[tgt] = loadmodel('LSTMModel',tgt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8024965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_features_build(p_test_df_feature,target_col1,target_col2):\n",
    "# Filter for the two targets\n",
    "    if target_col2 is not None:\n",
    "        df_target2 = p_test_df_feature[p_test_df_feature['id'] == target_col2]\n",
    "        df_target1 = p_test_df_feature[p_test_df_feature['id'] == target_col1]\n",
    "        \n",
    "        # Concatenate the two DataFrames side by side (axis=1)\n",
    "        result = df_target2.merge(\n",
    "    df_target1,\n",
    "    on='date',\n",
    "    how='left',   # ou 'left'\n",
    "    suffixes=('_t2', '_t1'))\n",
    "        result = result.reset_index()\n",
    "        result = result.drop(['id_t2', 'date_t2','id_t1', 'date_t1', 'index', 'date'], axis=1, errors='ignore')\n",
    "        \n",
    "        return result\n",
    "    else:\n",
    "    \n",
    "        df_target1 = p_test_df_feature[p_test_df_feature['id'] == target_col1]\n",
    "        # Concatenate the two DataFrames side by side (axis=1)\n",
    "        result = df_target1\n",
    "        result = result.reset_index()\n",
    "        result = result.drop(['id', 'date', 'index'], axis=1, errors='ignore')\n",
    "        \n",
    "        return result\n",
    "    \n",
    "def update_and_predict(x_new, model, scaler, device=None):\n",
    "    \"\"\"\n",
    "    param x_new: nouvelle fonction\n",
    "    param model :\n",
    "    param scaler:\n",
    "    \"\"\"\n",
    "\n",
    "    # scaler\n",
    "    X_scaled = scaler.transform(x_new)\n",
    "    X_tensor = torch.tensor(X_scaled, dtype=torch.float32).unsqueeze(0)\n",
    "    \n",
    "    if device is not None:\n",
    "        X_tensor = X_tensor.to(device)\n",
    "        model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(X_tensor)\n",
    "\n",
    "    return pred.cpu().numpy()[0, 0]\n",
    "\n",
    "sequence_buffer_gru={}\n",
    "sequence_buffer_lstm={}\n",
    "\n",
    "def update_buffer(key, x_new, buffer_dict, sequence_length):\n",
    "    \"\"\"\n",
    "    key : identifiant de la série\n",
    "    x_new : pd.DataFrame( (n_features,)\n",
    "    buffer_dict : dict contenant les buffers\n",
    "    \"\"\"\n",
    "  \n",
    "    # si le key n'existe pas, on initialise le buffer avec des zéros\n",
    "    if key not in buffer_dict:\n",
    "        df_zero = pd.DataFrame(np.zeros((sequence_length, x_new.shape[1])),\n",
    "                       columns=x_new.columns)\n",
    "        buffer_dict[key] = df_zero\n",
    "\n",
    "    # décaler le buffer\n",
    "    buffer_dict[key] = buffer_dict[key].shift(-1)\n",
    "    buffer_dict[key].iloc[-1] = x_new.values\n",
    "\n",
    "    return buffer_dict[key]  # renvoie la séquence mise à jour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "c1b3442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_batch):\n",
    "    print(\"\\nGenerating predictions...\")\n",
    "    print(\"=\" * 50)\n",
    "    test_batch, label_lags_1_batch, label_lags_2_batch, label_lags_3_batch, label_lags_4_batch = data_batch\n",
    "\n",
    "    # Get the target columns from lag data\n",
    "    lag_1_values = label_lags_1_batch.drop(['date_id', 'label_date_id'])\n",
    "    lag_2_values = label_lags_2_batch.drop(['date_id', 'label_date_id'])\n",
    "    lag_3_values = label_lags_3_batch.drop(['date_id', 'label_date_id'])\n",
    "    lag_4_values = label_lags_4_batch.drop(['date_id', 'label_date_id'])\n",
    "  \n",
    "    df_label_lags = pl.concat([lag_1_values, lag_2_values,lag_3_values, lag_4_values], how=\"horizontal\")\n",
    "    lag_values = df_label_lags.to_pandas()\n",
    "    target_columns = lag_values.columns.tolist()\n",
    "\n",
    "    # Generate ensemble predictions\n",
    "    predictions = pd.DataFrame()\n",
    "    sequence_length = 10\n",
    "\n",
    "    # effectue les préditions\n",
    "    index: int = 0\n",
    "    for target_col in target_columns:\n",
    "        ensemble_preds = []\n",
    "        name,pric1,pric2 =getnameprice(index)\n",
    "        \n",
    "        if index >= 2:\n",
    "            continue\n",
    "\n",
    "        X_test  = test_features_build(test_batch, pric1,pric2)\n",
    "\n",
    "        # Cas du GRU  \n",
    "        sequence = update_buffer(index, X_test, sequence_buffer_gru, sequence_length)\n",
    "        pred = update_and_predict(sequence, models_gru[target_col],scalers_gru[target_col],device)\n",
    "        ensemble_preds.append(pred)\n",
    "\n",
    "        # Cas du LSTM  \n",
    "        sequence = update_buffer(index, X_test, sequence_buffer_lstm, sequence_length)\n",
    "        pred = update_and_predict(sequence, models_lstm[target_col],scalers_lstm[target_col],device)\n",
    "        ensemble_preds.append(pred)\n",
    "\n",
    "        index += 1\n",
    "\n",
    "        if ensemble_preds:\n",
    "            # Average the predictions\n",
    "            final_pred = np.mean(ensemble_preds)\n",
    "            predictions[target_col] = [final_pred]\n",
    "            \n",
    "        else:\n",
    "            # Fallback to lag-based prediction\n",
    "            lag_val = lag_values[target_col].iloc[0] if len(lag_values) > 0 else 0.0\n",
    "            predictions[target_col] = [lag_val + np.random.normal(0, 0.001)]\n",
    "        \n",
    "    # Ensure all values are finite\n",
    "    predictions = predictions.fillna(0.0)\n",
    "    predictions = predictions.replace([np.inf, -np.inf], 0.0)\n",
    "    \n",
    "    return predictions, df_label_lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9219c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de score entre la prédiction et l'objectif\n",
    "\n",
    "def score(prediction:pd.DataFrame, target:pd.DataFrame) -> float: \n",
    "     return np.mean(np.abs(target.values - prediction.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8a8f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1827\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1828\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1829\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1830\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1831\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1832\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1833\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1834\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1835\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1836\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1837\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1838\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1839\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1840\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1841\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1842\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1843\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1844\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1845\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1846\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1847\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1848\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1849\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1850\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1851\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1852\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1853\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1854\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1855\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1856\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1857\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1858\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1859\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1860\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1861\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1862\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1863\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1864\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1865\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1866\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1867\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1868\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1869\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1870\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1871\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1872\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1873\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1874\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1875\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1876\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1877\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1878\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1879\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1880\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1881\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1882\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1883\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1884\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1885\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1886\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1887\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1888\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1889\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1890\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1891\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1892\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1893\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1894\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1895\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1896\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1897\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1898\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1899\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1900\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1901\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1902\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1903\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1904\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1905\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1906\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1907\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1908\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1909\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1910\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1911\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1912\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1913\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1914\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1915\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n",
      "1916\n",
      "\n",
      "Generating predictions...\n",
      "==================================================\n",
      "(1, 424)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (90,425) (90,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[215]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m submission_df = pd.concat(submissions, ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     24\u001b[39m submission_df.to_csv(\u001b[33m'\u001b[39m\u001b[33m../outputs/submission.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m display(\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubmission_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarget_df\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[212]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mscore\u001b[39m\u001b[34m(prediction, target)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mscore\u001b[39m(prediction:pd.DataFrame, target:pd.DataFrame) -> \u001b[38;5;28mfloat\u001b[39m: \n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m      \u001b[38;5;28;01mreturn\u001b[39;00m np.mean(np.abs(\u001b[43mtarget\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m))\n",
      "\u001b[31mValueError\u001b[39m: operands could not be broadcast together with shapes (90,425) (90,3) "
     ]
    }
   ],
   "source": [
    "counter_batch = generate_data_batches()\n",
    "submissions = []\n",
    "target = []\n",
    "for data_batch, date_id in counter_batch:\n",
    "    print (date_id)\n",
    "    predictions, df_label_lags = predict(data_batch)\n",
    "   \n",
    "    print (df_label_lags.shape)\n",
    "    predictions = predictions.copy()\n",
    "    predictions[\"date_id\"] = date_id\n",
    "   \n",
    "    # predictions = predictions[train_labels_df.columns]\n",
    "    predictions = predictions[[\"date_id\", \"target_0\", \"target_1\"]]\n",
    "\n",
    "    df_label_lags = df_label_lags.with_columns( pl.lit(date_id).alias(\"date_id\"))\n",
    "    submissions.append(predictions)\n",
    "    target.append(df_label_lags.to_pandas())\n",
    "\n",
    "target_df = pd.concat(target, ignore_index=True)\n",
    "target_df.to_csv('../outputs/target.csv', index=False)\n",
    "\n",
    "submission_df = pd.concat(submissions, ignore_index=True)\n",
    "submission_df.to_csv('../outputs/submission.csv', index=False)\n",
    "\n",
    "display(score(submission_df,target_df))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
